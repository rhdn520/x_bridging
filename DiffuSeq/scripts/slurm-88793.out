 OPENAI_LOGDIR=diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-23:17:04  TOKENIZERS_PARALLELISM=false python train.py   --checkpoint_path diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-23:17:04 --dataset qqp --data_dir /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp --vocab bert --use_plm_init bert --lr 0.0001 --batch_size 1024 --microbatch 64 --diffusion_steps 2000 --noise_schedule sqrt --schedule_sampler lossaware --resume_checkpoint none --seq_len 128 --hidden_t_dim 128 --seed 102 --hidden_dim 128 --learning_steps 50000 --save_interval 10000 --config_name bert-base-uncased --notes test-qqp20251118-23:17:04 
Logging to diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-23:17:04
### Creating data loader...
initializing the random embeddings Embedding(30522, 128)
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the TRAIN set...
### Data samples...
 ['Academic and Educational Advice: What can I do after completing bcom?', 'What are some good songs to make a texting lyric prank?'] ['What should I do after bcom?', 'What is a good prank lyric text to text to a friend?']
RAM used: 947.23 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 144715
})
RAM used: 994.28 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):   1%|          | 1000/144715 [00:00<01:12, 1972.71 examples/s]Running tokenizer on dataset (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:25, 5458.93 examples/s]Running tokenizer on dataset (num_proc=1):   3%|â–Ž         | 5000/144715 [00:00<00:17, 8067.44 examples/s]Running tokenizer on dataset (num_proc=1):   5%|â–         | 7000/144715 [00:00<00:13, 9849.64 examples/s]Running tokenizer on dataset (num_proc=1):   6%|â–Œ         | 9000/144715 [00:01<00:12, 11187.09 examples/s]Running tokenizer on dataset (num_proc=1):   8%|â–Š         | 11000/144715 [00:01<00:14, 9233.70 examples/s]Running tokenizer on dataset (num_proc=1):   9%|â–‰         | 13000/144715 [00:01<00:12, 10370.07 examples/s]Running tokenizer on dataset (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:01<00:11, 11353.21 examples/s]Running tokenizer on dataset (num_proc=1):  12%|â–ˆâ–        | 17000/144715 [00:01<00:10, 12124.20 examples/s]Running tokenizer on dataset (num_proc=1):  13%|â–ˆâ–Ž        | 19000/144715 [00:01<00:09, 12658.94 examples/s]Running tokenizer on dataset (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:02<00:12, 10021.84 examples/s]Running tokenizer on dataset (num_proc=1):  16%|â–ˆâ–Œ        | 23000/144715 [00:02<00:11, 11018.18 examples/s]Running tokenizer on dataset (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:02<00:10, 11827.12 examples/s]Running tokenizer on dataset (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:02<00:09, 12463.92 examples/s]Running tokenizer on dataset (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:08, 12965.40 examples/s]Running tokenizer on dataset (num_proc=1):  21%|â–ˆâ–ˆâ–       | 31000/144715 [00:02<00:08, 13309.92 examples/s]Running tokenizer on dataset (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:03<00:10, 10278.13 examples/s]Running tokenizer on dataset (num_proc=1):  24%|â–ˆâ–ˆâ–       | 35000/144715 [00:03<00:09, 11233.05 examples/s]Running tokenizer on dataset (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:03<00:09, 11922.76 examples/s]Running tokenizer on dataset (num_proc=1):  27%|â–ˆâ–ˆâ–‹       | 39000/144715 [00:03<00:08, 12491.54 examples/s]Running tokenizer on dataset (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 41000/144715 [00:03<00:08, 12919.50 examples/s]Running tokenizer on dataset (num_proc=1):  30%|â–ˆâ–ˆâ–‰       | 43000/144715 [00:04<00:09, 10233.63 examples/s]Running tokenizer on dataset (num_proc=1):  31%|â–ˆâ–ˆâ–ˆ       | 45000/144715 [00:04<00:08, 11137.07 examples/s]Running tokenizer on dataset (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 47000/144715 [00:04<00:08, 11900.35 examples/s]Running tokenizer on dataset (num_proc=1):  34%|â–ˆâ–ˆâ–ˆâ–      | 49000/144715 [00:04<00:07, 12468.18 examples/s]Running tokenizer on dataset (num_proc=1):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 51000/144715 [00:04<00:07, 13118.76 examples/s]Running tokenizer on dataset (num_proc=1):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 53000/144715 [00:04<00:06, 13830.73 examples/s]Running tokenizer on dataset (num_proc=1):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 55000/144715 [00:05<00:07, 11248.00 examples/s]Running tokenizer on dataset (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 57000/144715 [00:05<00:07, 12334.88 examples/s]Running tokenizer on dataset (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:05<00:06, 13245.33 examples/s]Running tokenizer on dataset (num_proc=1):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 61000/144715 [00:05<00:06, 13900.77 examples/s]Running tokenizer on dataset (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:05<00:05, 14477.92 examples/s]Running tokenizer on dataset (num_proc=1):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65000/144715 [00:05<00:06, 11858.29 examples/s]Running tokenizer on dataset (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:05<00:06, 12825.51 examples/s]Running tokenizer on dataset (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69000/144715 [00:06<00:05, 13575.05 examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:06<00:05, 14217.21 examples/s]Running tokenizer on dataset (num_proc=1):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73000/144715 [00:06<00:04, 14451.89 examples/s]Running tokenizer on dataset (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:06<00:05, 11840.14 examples/s]Running tokenizer on dataset (num_proc=1):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 77000/144715 [00:06<00:05, 12845.91 examples/s]Running tokenizer on dataset (num_proc=1):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79000/144715 [00:06<00:04, 13631.06 examples/s]Running tokenizer on dataset (num_proc=1):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 81000/144715 [00:06<00:04, 14259.44 examples/s]Running tokenizer on dataset (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 83000/144715 [00:07<00:04, 14657.48 examples/s]Running tokenizer on dataset (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 85000/144715 [00:07<00:05, 11585.88 examples/s]Running tokenizer on dataset (num_proc=1):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 87000/144715 [00:07<00:04, 12594.07 examples/s]Running tokenizer on dataset (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:07<00:04, 13205.45 examples/s]Running tokenizer on dataset (num_proc=1):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 91000/144715 [00:07<00:04, 13391.77 examples/s]Running tokenizer on dataset (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:07<00:03, 13624.35 examples/s]Running tokenizer on dataset (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 95000/144715 [00:07<00:03, 13759.04 examples/s]Running tokenizer on dataset (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:08<00:04, 10554.30 examples/s]Running tokenizer on dataset (num_proc=1):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 99000/144715 [00:08<00:04, 11353.68 examples/s]Running tokenizer on dataset (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:08<00:03, 12047.59 examples/s]Running tokenizer on dataset (num_proc=1):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 103000/144715 [00:08<00:03, 12629.31 examples/s]Running tokenizer on dataset (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:08<00:03, 13104.17 examples/s]Running tokenizer on dataset (num_proc=1):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107000/144715 [00:09<00:03, 10376.06 examples/s]Running tokenizer on dataset (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:09<00:03, 11275.08 examples/s]Running tokenizer on dataset (num_proc=1):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 111000/144715 [00:09<00:02, 12103.84 examples/s]Running tokenizer on dataset (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:09<00:02, 12718.45 examples/s]Running tokenizer on dataset (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 115000/144715 [00:09<00:02, 13253.85 examples/s]Running tokenizer on dataset (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:09<00:02, 13479.30 examples/s]Running tokenizer on dataset (num_proc=1):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 119000/144715 [00:10<00:02, 10562.56 examples/s]Running tokenizer on dataset (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:10<00:02, 11503.22 examples/s]Running tokenizer on dataset (num_proc=1):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123000/144715 [00:10<00:01, 12283.27 examples/s]Running tokenizer on dataset (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:10<00:01, 12843.58 examples/s]Running tokenizer on dataset (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 127000/144715 [00:10<00:01, 13216.43 examples/s]Running tokenizer on dataset (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:10<00:01, 10521.90 examples/s]Running tokenizer on dataset (num_proc=1):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 131000/144715 [00:11<00:01, 11263.99 examples/s]Running tokenizer on dataset (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:11<00:00, 12004.61 examples/s]Running tokenizer on dataset (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 135000/144715 [00:11<00:00, 12473.99 examples/s]Running tokenizer on dataset (num_proc=1):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137000/144715 [00:11<00:00, 12947.65 examples/s]Running tokenizer on dataset (num_proc=1):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 139000/144715 [00:11<00:00, 10304.10 examples/s]Running tokenizer on dataset (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 141000/144715 [00:11<00:00, 11270.11 examples/s]Running tokenizer on dataset (num_proc=1):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 143000/144715 [00:12<00:00, 12036.20 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:12<00:00, 12593.39 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:13<00:00, 10711.58 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 144715
})
### tokenized_datasets...example [101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102]
RAM used: 1111.26 MB
merge and mask (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]merge and mask (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:24, 5769.23 examples/s]merge and mask (num_proc=1):   6%|â–Œ         | 9000/144715 [00:00<00:07, 17214.59 examples/s]merge and mask (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:00<00:04, 27024.03 examples/s]merge and mask (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:00<00:03, 34614.63 examples/s]merge and mask (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:00<00:02, 40707.56 examples/s]merge and mask (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:01<00:02, 45539.06 examples/s]merge and mask (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:01<00:03, 32809.36 examples/s]merge and mask (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 46000/144715 [00:01<00:02, 38085.59 examples/s]merge and mask (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:01<00:02, 42327.24 examples/s]merge and mask (num_proc=1):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58000/144715 [00:01<00:01, 46302.12 examples/s]merge and mask (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64000/144715 [00:01<00:01, 49324.31 examples/s]merge and mask (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 70000/144715 [00:01<00:01, 51782.24 examples/s]merge and mask (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:02<00:01, 39467.02 examples/s]merge and mask (num_proc=1):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84000/144715 [00:02<00:01, 43244.51 examples/s]merge and mask (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 90000/144715 [00:02<00:01, 46667.72 examples/s]merge and mask (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96000/144715 [00:02<00:00, 49251.21 examples/s]merge and mask (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 102000/144715 [00:02<00:00, 51560.46 examples/s]merge and mask (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108000/144715 [00:02<00:00, 52227.50 examples/s]merge and mask (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114000/144715 [00:02<00:00, 53761.34 examples/s]merge and mask (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122000/144715 [00:03<00:00, 42411.59 examples/s]merge and mask (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 128000/144715 [00:03<00:00, 45881.38 examples/s]merge and mask (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134000/144715 [00:03<00:00, 48953.83 examples/s]merge and mask (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:03<00:00, 50636.63 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:05<00:00, 28358.70 examples/s]
RAM used: 1190.80 MB
padding (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1123875.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1495828.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1206299.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1544863.35it/s]
padding (num_proc=1):   1%|â–         | 2000/144715 [00:00<00:55, 2581.72 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1230001.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1569137.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1253153.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1510916.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1244600.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1557483.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1307043.94it/s]

0it [00:00, ?it/s][A1000it [00:00, 1492103.88it/s]
padding (num_proc=1):   4%|â–         | 6000/144715 [00:00<00:18, 7511.01 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1273316.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1573847.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1253153.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1493166.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1256155.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1560961.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1259172.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497430.92it/s]
padding (num_proc=1):   7%|â–‹         | 10000/144715 [00:01<00:11, 11454.75 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1283839.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1544294.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1216798.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1567378.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1236163.87it/s]

0it [00:00, ?it/s][A1000it [00:00, 1525756.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1279921.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1557483.85it/s]
padding (num_proc=1):  10%|â–‰         | 14000/144715 [00:01<00:08, 14642.30 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1272543.69it/s]

0it [00:00, ?it/s][A1000it [00:00, 1512005.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1293739.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1576806.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1253527.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497965.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1198372.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1484709.38it/s]
padding (num_proc=1):  12%|â–ˆâ–        | 18000/144715 [00:01<00:07, 17057.15 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1284626.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497430.92it/s]

0it [00:00, ?it/s][A1000it [00:00, 1279531.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1577399.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1278751.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1080449.25it/s]
padding (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:01<00:09, 13177.58 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1239085.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1543158.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1220693.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481562.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1214332.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497965.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1115803.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448809.67it/s]
padding (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:01<00:07, 15474.51 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1230361.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1507116.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1246449.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477387.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1255403.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1542590.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1195298.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481562.70it/s]
padding (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:06, 17404.96 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1189873.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477387.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1237622.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1524647.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1226046.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1552296.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1186507.50it/s]

0it [00:00, ?it/s][A1000it [00:00, 1527423.16it/s]
padding (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:02<00:05, 18973.77 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1218920.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1543726.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1212226.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1515283.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1212226.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1501182.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1288968.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1547142.75it/s]
padding (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:02<00:05, 20203.53 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1276416.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1554021.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1162501.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1261444.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1255028.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1543726.17it/s]
padding (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:02<00:05, 19222.32 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1233981.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1545432.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1293340.73it/s]

0it [00:00, ?it/s][A1000it [00:00, 1535250.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1185166.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1530209.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1150385.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1182493.37it/s]
padding (num_proc=1):  30%|â–ˆâ–ˆâ–ˆ       | 44000/144715 [00:02<00:04, 20255.51 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1263725.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461939.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1273316.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1519675.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1283054.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1567378.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1196321.73it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483659.00it/s]
padding (num_proc=1):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 48000/144715 [00:03<00:04, 21240.75 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 986198.92it/s]

0it [00:00, ?it/s][A1000it [00:00, 1492103.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1281094.69it/s]

0it [00:00, ?it/s][A1000it [00:00, 1555173.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1260307.69it/s]

0it [00:00, ?it/s][A1000it [00:00, 1560380.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1241653.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1502257.88it/s]
padding (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:03<00:04, 21909.20 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1210826.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1468079.80it/s]

0it [00:00, ?it/s][A1000it [00:00, 1205606.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1479994.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1210826.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1535812.52it/s]

0it [00:00, ?it/s][A1000it [00:00, 1292942.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1542023.53it/s]
padding (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 56000/144715 [00:03<00:03, 22382.70 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1277193.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 930000.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1239085.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1550001.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1275252.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1532445.74it/s]
padding (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:03<00:04, 17259.59 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1255028.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1552296.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1279921.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1549428.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1233981.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1545432.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218920.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1551147.93it/s]
padding (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:03<00:04, 18692.46 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1263725.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1558062.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1276027.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1484184.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1263344.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483659.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1241653.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1534688.62it/s]
padding (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:04<00:03, 20006.01 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1309083.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1544294.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1274089.91it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497965.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1222829.15it/s]

0it [00:00, ?it/s][A1000it [00:00, 1544863.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1266778.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1563288.86it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:04<00:03, 20880.27 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1217151.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1562124.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1304199.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1559220.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1269462.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1546002.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1237257.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483134.37it/s]
padding (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:04<00:03, 21621.06 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1197004.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1422280.09it/s]

0it [00:00, ?it/s][A1000it [00:00, 1250910.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1401371.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1200774.12it/s]

0it [00:00, ?it/s][A1000it [00:00, 1485761.25it/s]
padding (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:04<00:03, 18159.96 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1230361.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1504953.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1258039.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1579180.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1313593.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1567378.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1233618.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1527979.60it/s]
padding (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82000/144715 [00:04<00:03, 19764.56 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1311949.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1583951.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1257662.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1510916.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1241285.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1501182.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1251657.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460412.26it/s]
padding (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 86000/144715 [00:04<00:02, 20871.35 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1286991.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1569724.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1276027.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1524647.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1273703.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1422762.55it/s]
padding (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:05<00:03, 17182.05 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1194277.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1571488.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1280703.51it/s]

0it [00:00, ?it/s][A1000it [00:00, 1534688.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1319793.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1506574.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1306229.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1586347.96it/s]
padding (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:05<00:02, 19010.19 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1080171.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1584550.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1274477.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1482086.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1311949.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1524093.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1304199.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1556328.01it/s]
padding (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:05<00:02, 20501.35 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1295337.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1582159.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1267544.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1511460.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1227122.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1528536.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1270231.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1570900.37it/s]
padding (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:05<00:02, 21629.11 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1319378.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1577399.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1049888.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1573257.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1304604.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1574438.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1252404.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1506033.75it/s]
padding (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:05<00:01, 22386.91 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1256155.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1525201.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1286201.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1573257.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1267161.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1473754.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1313182.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1506574.71it/s]
padding (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:06<00:01, 23069.49 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1259929.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1568550.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1255779.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461939.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1309901.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1576213.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1306636.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1555173.90it/s]
padding (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:06<00:01, 23520.59 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1323959.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475309.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1278751.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1562124.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1292145.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1519675.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1240551.32it/s]

0it [00:00, ?it/s][A1000it [00:00, 1490513.15it/s]
padding (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:06<00:01, 23849.47 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1314005.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1571488.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1314416.80it/s]

0it [00:00, ?it/s][A1000it [00:00, 1570312.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1275252.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1487869.46it/s]

0it [00:00, ?it/s][A1000it [00:00, 1331947.92it/s]

0it [00:00, ?it/s][A1000it [00:00, 1585148.90it/s]
padding (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:06<00:00, 24208.45 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1269846.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1509285.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1252031.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1508199.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1286201.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1502796.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1290952.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1579775.52it/s]
padding (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:06<00:00, 24159.48 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1321040.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1556905.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1307451.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1559220.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1230723.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1508742.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1254652.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1506574.71it/s]
padding (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:06<00:00, 24390.07 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1317720.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1556905.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1306636.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1580966.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1340461.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1569137.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1027008.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1510916.43it/s]
padding (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:06<00:00, 24481.38 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1198372.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1492103.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1246820.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1565622.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1018282.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1443822.38it/s]
padding (num_proc=1):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 136000/144715 [00:07<00:00, 19053.45 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1259550.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1501720.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1259929.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1492103.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1297742.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1576213.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1311949.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1568550.49it/s]
padding (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:07<00:00, 20567.06 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1293739.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1555750.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1273316.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1452824.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1143485.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1418912.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1219983.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1518025.33it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 144000/144715 [00:07<00:00, 21397.77 examples/s]
0it [00:00, ?it/s][A715it [00:00, 1225552.66it/s]

0it [00:00, ?it/s][A715it [00:00, 1484617.50it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:11<00:00, 12155.46 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 144715
}) padded dataset
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102]])
Column([[101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102], [101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102], [101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102], [101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102]])
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102, 102, 101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102, 102, 101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102, 102, 101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 102, 101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102, 102, 101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1685.11 MB
RAM used: 1685.11 MB
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the VALID set...
### Data samples...
 ['Why does he want to have sex with me not her?', 'When/how did you realize were not straight?'] ['Why did he chose me to have sex with?', 'When/how did you realize you were gay/bisexual? Were you in denial?']
RAM used: 1603.10 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2048
})
RAM used: 1560.05 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 1237.60 examples/s]Running tokenizer on dataset (num_proc=1):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2000/2048 [00:02<00:00, 910.63 examples/s] Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:02<00:00, 803.69 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2048
})
### tokenized_datasets...example [101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102]
RAM used: 1563.41 MB
merge and mask (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]merge and mask (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 4421.80 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 4099.07 examples/s]
RAM used: 1565.52 MB
padding (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1151016.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1580966.45it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 3559.16 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1303793.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1565038.81it/s]

0it [00:00, ?it/s][A48it [00:00, 742902.55it/s]

0it [00:00, ?it/s][A48it [00:00, 932067.56it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 3389.91 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2048
}) padded dataset
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102]])
Column([[101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102], [101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102], [101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102], [101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102]])
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102, 102, 101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102, 102, 101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102, 102, 101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102, 102, 101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102, 102, 101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1568.48 MB
RAM used: 1568.48 MB
############################## size of vocab 30522
### Creating model and diffusion...
dict_keys(['lr', 'batch_size', 'microbatch', 'learning_steps', 'log_interval', 'save_interval', 'eval_interval', 'ema_rate', 'resume_checkpoint', 'schedule_sampler', 'diffusion_steps', 'noise_schedule', 'timestep_respacing', 'vocab', 'use_plm_init', 'vocab_size', 'config_name', 'notes', 'data_dir', 'dataset', 'checkpoint_path', 'seq_len', 'hidden_t_dim', 'hidden_dim', 'dropout', 'use_fp16', 'fp16_scale_growth', 'seed', 'gradient_clipping', 'weight_decay', 'learn_sigma', 'use_kl', 'predict_xstart', 'rescale_timesteps', 'rescale_learned_sigmas', 'sigma_small', 'emb_scale_factor'])
initializing from pretrained bert...
BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.57.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

### The parameter count is 110759354
### Saving the hyperparameters to diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-23:17:04/training_args.json
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /data6/seungwoochoi/x_bridging/DiffuSeq/wandb/offline-run-20251118_231812-c4ihuv2d
### Training...
cuda:0
x_start_mean::::
torch.Size([64, 128, 768])
Traceback (most recent call last):
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 116, in <module>
    main()
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 113, in main
    ).run_loop()
      ^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 179, in run_loop
    self.run_step(batch, cond)
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 198, in run_step
    self.forward_backward(batch, cond)
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 260, in forward_backward
    losses = compute_losses()
             ^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 1014, in training_losses
    return super().training_losses(self._wrap_model(model), *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 196, in training_losses
    return self.training_losses_seq2seq(model, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 694, in training_losses_seq2seq
    model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 1046, in __call__
    return self.model(x, new_ts, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1661, in forward
    else self._run_ddp_forward(*inputs, **kwargs)
         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1487, in _run_ddp_forward
    return self.module(*inputs, **kwargs)  # type: ignore[index]
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/transformer_model.py", line 140, in forward
    emb_x = self.input_up_proj(x)
            ^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
            ^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 134, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: mat1 and mat2 shapes cannot be multiplied (8192x768 and 128x768)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 116, in <module>
[rank0]:     main()
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 113, in main
[rank0]:     ).run_loop()
[rank0]:       ^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 179, in run_loop
[rank0]:     self.run_step(batch, cond)
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 198, in run_step
[rank0]:     self.forward_backward(batch, cond)
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 260, in forward_backward
[rank0]:     losses = compute_losses()
[rank0]:              ^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 1014, in training_losses
[rank0]:     return super().training_losses(self._wrap_model(model), *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 196, in training_losses
[rank0]:     return self.training_losses_seq2seq(model, *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 694, in training_losses_seq2seq
[rank0]:     model_output = model(x_t, self._scale_timesteps(t), **model_kwargs)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 1046, in __call__
[rank0]:     return self.model(x, new_ts, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1661, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/parallel/distributed.py", line 1487, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/transformer_model.py", line 140, in forward
[rank0]:     emb_x = self.input_up_proj(x)
[rank0]:             ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/container.py", line 250, in forward
[rank0]:     input = module(input)
[rank0]:             ^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/linear.py", line 134, in forward
[rank0]:     return F.linear(input, self.weight, self.bias)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: mat1 and mat2 shapes cannot be multiplied (8192x768 and 128x768)
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /data6/seungwoochoi/x_bridging/DiffuSeq/wandb/offline-run-20251118_231812-c4ihuv2d[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20251118_231812-c4ihuv2d/logs[0m
