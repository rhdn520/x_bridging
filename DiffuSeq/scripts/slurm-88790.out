 OPENAI_LOGDIR=diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-22:48:53  TOKENIZERS_PARALLELISM=false python train.py   --checkpoint_path diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-22:48:53 --dataset qqp --data_dir /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp --vocab bert --use_plm_init no --lr 0.0001 --batch_size 1024 --microbatch 64 --diffusion_steps 2000 --noise_schedule sqrt --schedule_sampler lossaware --resume_checkpoint none --seq_len 128 --hidden_t_dim 128 --seed 102 --hidden_dim 128 --learning_steps 50000 --save_interval 10000 --config_name bert-base-uncased --notes test-qqp20251118-22:48:53 
Logging to diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-22:48:53
### Creating data loader...
initializing the random embeddings Embedding(30522, 128)
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the TRAIN set...
### Data samples...
 ['Academic and Educational Advice: What can I do after completing bcom?', 'What are some good songs to make a texting lyric prank?'] ['What should I do after bcom?', 'What is a good prank lyric text to text to a friend?']
RAM used: 948.16 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 144715
})
RAM used: 995.04 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):   1%|          | 1000/144715 [00:00<01:09, 2065.19 examples/s]Running tokenizer on dataset (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:24, 5756.89 examples/s]Running tokenizer on dataset (num_proc=1):   3%|â–Ž         | 5000/144715 [00:00<00:16, 8593.13 examples/s]Running tokenizer on dataset (num_proc=1):   5%|â–         | 7000/144715 [00:00<00:12, 10643.32 examples/s]Running tokenizer on dataset (num_proc=1):   6%|â–Œ         | 9000/144715 [00:00<00:11, 12170.21 examples/s]Running tokenizer on dataset (num_proc=1):   8%|â–Š         | 11000/144715 [00:01<00:12, 10319.84 examples/s]Running tokenizer on dataset (num_proc=1):   9%|â–‰         | 13000/144715 [00:01<00:11, 11641.44 examples/s]Running tokenizer on dataset (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:01<00:10, 12586.28 examples/s]Running tokenizer on dataset (num_proc=1):  12%|â–ˆâ–        | 17000/144715 [00:01<00:09, 13519.01 examples/s]Running tokenizer on dataset (num_proc=1):  13%|â–ˆâ–Ž        | 19000/144715 [00:01<00:08, 14129.62 examples/s]Running tokenizer on dataset (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:01<00:10, 11459.82 examples/s]Running tokenizer on dataset (num_proc=1):  16%|â–ˆâ–Œ        | 23000/144715 [00:02<00:09, 12482.03 examples/s]Running tokenizer on dataset (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:02<00:09, 13193.00 examples/s]Running tokenizer on dataset (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:02<00:08, 13910.32 examples/s]Running tokenizer on dataset (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:08, 14296.43 examples/s]Running tokenizer on dataset (num_proc=1):  21%|â–ˆâ–ˆâ–       | 31000/144715 [00:02<00:07, 14676.27 examples/s]Running tokenizer on dataset (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:02<00:09, 11659.07 examples/s]Running tokenizer on dataset (num_proc=1):  24%|â–ˆâ–ˆâ–       | 35000/144715 [00:03<00:09, 12171.48 examples/s]Running tokenizer on dataset (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:03<00:08, 12573.44 examples/s]Running tokenizer on dataset (num_proc=1):  27%|â–ˆâ–ˆâ–‹       | 39000/144715 [00:03<00:08, 13028.20 examples/s]Running tokenizer on dataset (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 41000/144715 [00:03<00:07, 13332.24 examples/s]Running tokenizer on dataset (num_proc=1):  30%|â–ˆâ–ˆâ–‰       | 43000/144715 [00:03<00:09, 10429.66 examples/s]Running tokenizer on dataset (num_proc=1):  31%|â–ˆâ–ˆâ–ˆ       | 45000/144715 [00:03<00:08, 11335.54 examples/s]Running tokenizer on dataset (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 47000/144715 [00:04<00:08, 12097.30 examples/s]Running tokenizer on dataset (num_proc=1):  34%|â–ˆâ–ˆâ–ˆâ–      | 49000/144715 [00:04<00:07, 12521.12 examples/s]Running tokenizer on dataset (num_proc=1):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 51000/144715 [00:04<00:07, 13002.99 examples/s]Running tokenizer on dataset (num_proc=1):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 53000/144715 [00:04<00:06, 13383.20 examples/s]Running tokenizer on dataset (num_proc=1):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 55000/144715 [00:04<00:08, 10440.93 examples/s]Running tokenizer on dataset (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 57000/144715 [00:04<00:07, 11289.00 examples/s]Running tokenizer on dataset (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:05<00:07, 11941.19 examples/s]Running tokenizer on dataset (num_proc=1):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 61000/144715 [00:05<00:06, 12458.21 examples/s]Running tokenizer on dataset (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:05<00:06, 12797.49 examples/s]Running tokenizer on dataset (num_proc=1):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65000/144715 [00:05<00:07, 10145.34 examples/s]Running tokenizer on dataset (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:05<00:07, 11023.61 examples/s]Running tokenizer on dataset (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69000/144715 [00:05<00:06, 11652.50 examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:06<00:06, 12200.53 examples/s]Running tokenizer on dataset (num_proc=1):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73000/144715 [00:06<00:05, 12346.89 examples/s]Running tokenizer on dataset (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:06<00:06, 10035.55 examples/s]Running tokenizer on dataset (num_proc=1):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 77000/144715 [00:06<00:06, 11052.26 examples/s]Running tokenizer on dataset (num_proc=1):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79000/144715 [00:06<00:05, 11803.93 examples/s]Running tokenizer on dataset (num_proc=1):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 81000/144715 [00:06<00:05, 12347.10 examples/s]Running tokenizer on dataset (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 83000/144715 [00:07<00:04, 12701.23 examples/s]Running tokenizer on dataset (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 85000/144715 [00:07<00:05, 10199.66 examples/s]Running tokenizer on dataset (num_proc=1):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 87000/144715 [00:07<00:05, 10999.43 examples/s]Running tokenizer on dataset (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:07<00:04, 11746.92 examples/s]Running tokenizer on dataset (num_proc=1):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 91000/144715 [00:07<00:04, 12295.20 examples/s]Running tokenizer on dataset (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:07<00:04, 12666.77 examples/s]Running tokenizer on dataset (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 95000/144715 [00:08<00:03, 13016.64 examples/s]Running tokenizer on dataset (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:08<00:04, 10310.10 examples/s]Running tokenizer on dataset (num_proc=1):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 99000/144715 [00:08<00:04, 11289.50 examples/s]Running tokenizer on dataset (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:08<00:03, 11930.24 examples/s]Running tokenizer on dataset (num_proc=1):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 103000/144715 [00:08<00:03, 12398.91 examples/s]Running tokenizer on dataset (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:08<00:03, 12778.30 examples/s]Running tokenizer on dataset (num_proc=1):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107000/144715 [00:09<00:03, 10125.73 examples/s]Running tokenizer on dataset (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:09<00:03, 10983.73 examples/s]Running tokenizer on dataset (num_proc=1):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 111000/144715 [00:09<00:02, 11683.34 examples/s]Running tokenizer on dataset (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:09<00:02, 12191.56 examples/s]Running tokenizer on dataset (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 115000/144715 [00:09<00:02, 12627.56 examples/s]Running tokenizer on dataset (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:09<00:02, 13185.06 examples/s]Running tokenizer on dataset (num_proc=1):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 119000/144715 [00:10<00:02, 10736.30 examples/s]Running tokenizer on dataset (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:10<00:02, 11664.45 examples/s]Running tokenizer on dataset (num_proc=1):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123000/144715 [00:10<00:01, 12394.86 examples/s]Running tokenizer on dataset (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:10<00:01, 13243.16 examples/s]Running tokenizer on dataset (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 127000/144715 [00:10<00:01, 13955.99 examples/s]Running tokenizer on dataset (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:11<00:01, 11485.78 examples/s]Running tokenizer on dataset (num_proc=1):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 131000/144715 [00:11<00:01, 12506.34 examples/s]Running tokenizer on dataset (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:11<00:00, 13432.10 examples/s]Running tokenizer on dataset (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 135000/144715 [00:11<00:00, 14182.71 examples/s]Running tokenizer on dataset (num_proc=1):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137000/144715 [00:11<00:00, 14687.97 examples/s]Running tokenizer on dataset (num_proc=1):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 139000/144715 [00:11<00:00, 11940.85 examples/s]Running tokenizer on dataset (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 141000/144715 [00:11<00:00, 12755.04 examples/s]Running tokenizer on dataset (num_proc=1):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 143000/144715 [00:12<00:00, 13582.14 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:12<00:00, 14123.20 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:13<00:00, 10822.86 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 144715
})
### tokenized_datasets...example [101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102]
RAM used: 1111.81 MB
merge and mask (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]merge and mask (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:23, 5954.83 examples/s]merge and mask (num_proc=1):   6%|â–Œ         | 9000/144715 [00:00<00:07, 17545.77 examples/s]merge and mask (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:00<00:04, 27021.25 examples/s]merge and mask (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:00<00:03, 34200.13 examples/s]merge and mask (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:00<00:02, 39816.10 examples/s]merge and mask (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:01<00:02, 44161.85 examples/s]merge and mask (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:01<00:03, 31093.79 examples/s]merge and mask (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 46000/144715 [00:01<00:02, 35857.26 examples/s]merge and mask (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:01<00:02, 40241.73 examples/s]merge and mask (num_proc=1):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58000/144715 [00:01<00:01, 43964.46 examples/s]merge and mask (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64000/144715 [00:01<00:01, 46426.64 examples/s]merge and mask (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 70000/144715 [00:01<00:01, 49034.83 examples/s]merge and mask (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:02<00:01, 36728.93 examples/s]merge and mask (num_proc=1):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84000/144715 [00:02<00:01, 40462.48 examples/s]merge and mask (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 90000/144715 [00:02<00:01, 43796.82 examples/s]merge and mask (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96000/144715 [00:02<00:01, 46743.73 examples/s]merge and mask (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 102000/144715 [00:02<00:00, 48263.11 examples/s]merge and mask (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108000/144715 [00:02<00:00, 50162.06 examples/s]merge and mask (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114000/144715 [00:02<00:00, 51348.12 examples/s]merge and mask (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122000/144715 [00:03<00:00, 39196.08 examples/s]merge and mask (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 128000/144715 [00:03<00:00, 42233.30 examples/s]merge and mask (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134000/144715 [00:03<00:00, 45303.47 examples/s]merge and mask (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:03<00:00, 47572.72 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:05<00:00, 27561.19 examples/s]
RAM used: 1191.59 MB
padding (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1083799.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476347.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1001505.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1485235.13it/s]
padding (num_proc=1):   1%|â–         | 2000/144715 [00:00<00:56, 2525.55 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1258417.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475828.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1303388.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1504953.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1261065.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1539759.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1231807.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1507657.80it/s]
padding (num_proc=1):   4%|â–         | 6000/144715 [00:00<00:18, 7376.67 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1188188.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445813.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1269846.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1485235.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1245709.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1512551.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1244970.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1521880.99it/s]
padding (num_proc=1):   7%|â–‹         | 10000/144715 [00:01<00:11, 11302.85 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1258417.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1387923.23it/s]

0it [00:00, ?it/s][A1000it [00:00, 1225329.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1504953.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1238719.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1558641.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1233618.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1526867.13it/s]
padding (num_proc=1):  10%|â–‰         | 14000/144715 [00:01<00:09, 14463.93 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1259550.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1544294.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1282661.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1538629.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1238353.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1493166.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1136975.87it/s]

0it [00:00, ?it/s][A1000it [00:00, 1576213.45it/s]
padding (num_proc=1):  12%|â–ˆâ–        | 18000/144715 [00:01<00:07, 16973.33 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1242388.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1575621.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1299350.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1563871.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1307451.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1152914.79it/s]
padding (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:01<00:09, 13519.66 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1189873.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1488925.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1329836.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1502257.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1333641.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1567378.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1301770.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1550574.49it/s]
padding (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:01<00:07, 16091.50 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1326471.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1482610.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1310310.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1496362.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1282661.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1539759.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1235071.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1473754.04it/s]
padding (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:06, 18109.94 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1324795.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1554021.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1305416.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1558062.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1303793.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1334915.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1283054.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1565038.81it/s]
padding (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:02<00:05, 19828.49 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1325214.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1519675.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1269846.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483134.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1333218.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1542023.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1319793.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1560380.95it/s]
padding (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:02<00:05, 21216.18 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1332371.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1514189.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1249792.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1559800.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1300156.23it/s]

0it [00:00, ?it/s][A1000it [00:00, 1562124.39it/s]
padding (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:02<00:05, 19907.69 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1318548.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1550574.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1326052.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1571488.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1324795.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1540890.52it/s]

0it [00:00, ?it/s][A1000it [00:00, 1228200.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1506574.71it/s]
padding (num_proc=1):  30%|â–ˆâ–ˆâ–ˆ       | 44000/144715 [00:02<00:04, 21141.37 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1260686.50it/s]

0it [00:00, ?it/s][A1000it [00:00, 1587548.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1326891.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1561542.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1306636.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1521880.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1304604.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1574438.44it/s]
padding (num_proc=1):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 48000/144715 [00:02<00:04, 22220.15 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1271001.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1560961.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1062118.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1484709.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1303388.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1506033.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1316479.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1556905.72it/s]
padding (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:03<00:04, 22926.05 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1261824.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1485235.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1260686.50it/s]

0it [00:00, ?it/s][A1000it [00:00, 1554597.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1300559.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1374280.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1236163.87it/s]

0it [00:00, ?it/s][A1000it [00:00, 1449811.27it/s]
padding (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 56000/144715 [00:03<00:03, 23349.09 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1265250.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 623224.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1159288.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465003.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1277971.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1557483.85it/s]
padding (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:03<00:04, 18186.19 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1276027.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1496362.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1244231.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1496896.50it/s]

0it [00:00, ?it/s][A1000it [00:00, 1240918.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1558641.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1295337.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1569137.30it/s]
padding (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:03<00:04, 19808.16 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1228200.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497965.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218920.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1559220.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1257285.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1455850.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1251284.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1504413.20it/s]
padding (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:03<00:03, 20975.81 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1289364.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1579775.52it/s]

0it [00:00, ?it/s][A1000it [00:00, 1232169.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1525756.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1266013.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1557483.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1302174.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1569137.30it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:04<00:03, 21929.79 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1261444.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1558641.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1274864.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1557483.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1278361.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1535250.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1259172.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1508199.93it/s]
padding (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:04<00:03, 22677.70 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1285413.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1539759.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1233618.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497430.92it/s]

0it [00:00, ?it/s][A1000it [00:00, 1192919.23it/s]

0it [00:00, ?it/s][A1000it [00:00, 1533005.85it/s]
padding (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:04<00:03, 19009.32 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1235435.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1499572.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1221404.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475828.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1216798.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1455344.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1253527.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1526311.50it/s]
padding (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82000/144715 [00:04<00:03, 20191.36 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1290158.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1534688.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1259550.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460412.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1232531.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1488397.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1239817.91it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465515.02it/s]
padding (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 86000/144715 [00:04<00:02, 21066.28 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1156411.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1523539.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1210128.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1518574.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1217151.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1311949.95it/s]
padding (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:05<00:03, 16553.59 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1001983.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1536375.09it/s]

0it [00:00, ?it/s][A1000it [00:00, 1188188.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445314.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1265250.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1530209.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1174875.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1437389.99it/s]
padding (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:05<00:02, 18315.50 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1209430.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497430.92it/s]

0it [00:00, ?it/s][A1000it [00:00, 1190548.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1482086.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1275252.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1529651.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1184162.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1486814.60it/s]
padding (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:05<00:02, 19556.38 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1200430.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1275639.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1188861.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476347.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1214684.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1509828.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1155137.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1479472.31it/s]
padding (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:05<00:02, 20499.32 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1158327.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1544863.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1244600.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1542023.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1156730.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1470653.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1161535.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1491573.26it/s]
padding (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:05<00:01, 21215.29 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1270231.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1516378.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1143485.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1502257.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1274477.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1486287.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1190548.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1400903.14it/s]
padding (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:05<00:01, 21792.38 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1203530.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1453831.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1187515.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1535812.52it/s]

0it [00:00, ?it/s][A1000it [00:00, 1223899.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1502257.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1239817.91it/s]

0it [00:00, ?it/s][A1000it [00:00, 1493166.25it/s]
padding (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:06<00:01, 22227.57 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1286201.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1558641.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1220338.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478950.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1286991.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1533566.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1198030.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1414605.06it/s]
padding (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:06<00:01, 22519.44 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1263725.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1548856.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1203530.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1502796.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1267927.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1470138.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1210826.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1539759.18it/s]
padding (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:06<00:01, 22808.97 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1240918.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1512551.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1211526.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1525756.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1267927.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1565038.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1271771.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1446810.62it/s]
padding (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:06<00:00, 22979.91 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1244970.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1488925.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1183494.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477387.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1227840.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460920.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1167029.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1455850.05it/s]
padding (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:06<00:00, 22950.31 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1263344.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1527979.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1163468.52it/s]

0it [00:00, ?it/s][A1000it [00:00, 1382889.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1281877.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475828.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1247562.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1187179.17it/s]
padding (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:07<00:00, 23056.89 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1198372.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445314.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1212577.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1464491.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 921825.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1422762.55it/s]
padding (num_proc=1):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 136000/144715 [00:07<00:00, 17566.55 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1196321.73it/s]

0it [00:00, ?it/s][A1000it [00:00, 1458888.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1197004.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1474272.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1231807.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481039.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1266778.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1463469.64it/s]
padding (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:07<00:00, 19071.76 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1230723.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1488925.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218565.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1466539.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1266778.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1427605.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1204567.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1485761.25it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 144000/144715 [00:07<00:00, 20268.51 examples/s]
0it [00:00, ?it/s][A715it [00:00, 1220068.09it/s]

0it [00:00, ?it/s][A715it [00:00, 1465035.35it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:11<00:00, 12274.60 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 144715
}) padded dataset
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102]])
Column([[101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102], [101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102], [101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102], [101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102]])
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102, 102, 101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102, 102, 101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102, 102, 101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 102, 101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102, 102, 101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1686.24 MB
RAM used: 1686.24 MB
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the VALID set...
### Data samples...
 ['Why does he want to have sex with me not her?', 'When/how did you realize were not straight?'] ['Why did he chose me to have sex with?', 'When/how did you realize you were gay/bisexual? Were you in denial?']
RAM used: 1604.50 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2048
})
RAM used: 1561.46 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 1189.83 examples/s]Running tokenizer on dataset (num_proc=1):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2000/2048 [00:02<00:00, 844.70 examples/s] Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:02<00:00, 775.60 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2048
})
### tokenized_datasets...example [101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102]
RAM used: 1564.82 MB
merge and mask (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]merge and mask (num_proc=1):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2000/2048 [00:00<00:00, 6602.36 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 3899.82 examples/s]
RAM used: 1566.93 MB
padding (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 958698.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1577992.48it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 3421.09 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1218212.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483134.37it/s]

0it [00:00, ?it/s][A48it [00:00, 724196.37it/s]

0it [00:00, ?it/s][A48it [00:00, 958698.06it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 3273.76 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2048
}) padded dataset
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102]])
Column([[101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102], [101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102], [101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102], [101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102]])
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102, 102, 101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102, 102, 101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102, 102, 101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102, 102, 101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102, 102, 101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1569.82 MB
RAM used: 1569.82 MB
############################## size of vocab 30522
### Creating model and diffusion...
dict_keys(['lr', 'batch_size', 'microbatch', 'learning_steps', 'log_interval', 'save_interval', 'eval_interval', 'ema_rate', 'resume_checkpoint', 'schedule_sampler', 'diffusion_steps', 'noise_schedule', 'timestep_respacing', 'vocab', 'use_plm_init', 'vocab_size', 'config_name', 'notes', 'data_dir', 'dataset', 'checkpoint_path', 'seq_len', 'hidden_t_dim', 'hidden_dim', 'dropout', 'use_fp16', 'fp16_scale_growth', 'seed', 'gradient_clipping', 'weight_decay', 'learn_sigma', 'use_kl', 'predict_xstart', 'rescale_timesteps', 'rescale_learned_sigmas', 'sigma_small', 'emb_scale_factor'])
### The parameter count is 91225274
### Saving the hyperparameters to diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-22:48:53/training_args.json
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /data6/seungwoochoi/x_bridging/DiffuSeq/wandb/offline-run-20251118_224939-0g79g3hi
### Training...
cuda:0
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
------------------------
| grad_norm | 26.7     |
| loss      | 0.88     |
| loss_q0   | 0.876    |
| loss_q1   | 0.879    |
| loss_q2   | 0.883    |
| loss_q3   | 0.882    |
| mse       | 0.88     |
| mse_q0    | 0.876    |
| mse_q1    | 0.879    |
| mse_q2    | 0.883    |
| mse_q3    | 0.882    |
| nll       | 13.3     |
| nll_q0    | 13       |
| nll_q1    | 13.3     |
| nll_q2    | 13.6     |
| nll_q3    | 13.6     |
| samples   | 1.02e+03 |
| step      | 0        |
------------------------
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
eval on validation set
---------------------------
| eval_loss    | 0.552    |
| eval_loss_q0 | 0.547    |
| eval_loss_q1 | 0.549    |
| eval_loss_q2 | 0.552    |
| eval_loss_q3 | 0.56     |
| eval_mse     | 0.552    |
| eval_mse_q0  | 0.547    |
| eval_mse_q1  | 0.549    |
| eval_mse_q2  | 0.552    |
| eval_mse_q3  | 0.56     |
| eval_nll     | 4.91     |
| eval_nll_q0  | 4.91     |
| eval_nll_q1  | 4.92     |
| eval_nll_q2  | 4.84     |
| eval_nll_q3  | 4.97     |
---------------------------
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
slurmstepd-n02: error: *** JOB 88790 ON n02 CANCELLED AT 2025-11-18T22:50:38 ***
