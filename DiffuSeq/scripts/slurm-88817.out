 OPENAI_LOGDIR=diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:07:07  TOKENIZERS_PARALLELISM=false python train.py   --checkpoint_path diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:07:07 --dataset qqp --data_dir /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp --vocab bert --use_plm_init bert --lr 0.0001 --batch_size 1024 --microbatch 64 --diffusion_steps 2000 --noise_schedule sqrt --schedule_sampler lossaware --resume_checkpoint none --seq_len 128 --hidden_t_dim 128 --seed 102 --hidden_dim 768 --learning_steps 50000 --save_interval 10000 --config_name bert-base-uncased --notes test-qqp20251119-01:07:07 
Logging to diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:07:07
### Creating data loader...
initializing the random embeddings Embedding(30522, 768)
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the TRAIN set...
### Data samples...
 ['Academic and Educational Advice: What can I do after completing bcom?', 'What are some good songs to make a texting lyric prank?'] ['What should I do after bcom?', 'What is a good prank lyric text to text to a friend?']
RAM used: 1024.14 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 144715
})
RAM used: 1071.17 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):   1%|          | 1000/144715 [00:00<01:15, 1898.11 examples/s]Running tokenizer on dataset (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:27, 5240.58 examples/s]Running tokenizer on dataset (num_proc=1):   3%|â–Ž         | 5000/144715 [00:00<00:18, 7693.10 examples/s]Running tokenizer on dataset (num_proc=1):   5%|â–         | 7000/144715 [00:00<00:14, 9422.63 examples/s]Running tokenizer on dataset (num_proc=1):   6%|â–Œ         | 9000/144715 [00:01<00:12, 10674.89 examples/s]Running tokenizer on dataset (num_proc=1):   8%|â–Š         | 11000/144715 [00:01<00:15, 8783.98 examples/s]Running tokenizer on dataset (num_proc=1):   9%|â–‰         | 13000/144715 [00:01<00:13, 9955.33 examples/s]Running tokenizer on dataset (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:01<00:11, 10894.95 examples/s]Running tokenizer on dataset (num_proc=1):  12%|â–ˆâ–        | 17000/144715 [00:01<00:10, 11618.34 examples/s]Running tokenizer on dataset (num_proc=1):  13%|â–ˆâ–Ž        | 19000/144715 [00:02<00:10, 12141.90 examples/s]Running tokenizer on dataset (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:02<00:12, 9641.34 examples/s] Running tokenizer on dataset (num_proc=1):  16%|â–ˆâ–Œ        | 23000/144715 [00:02<00:11, 10545.10 examples/s]Running tokenizer on dataset (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:02<00:10, 11290.99 examples/s]Running tokenizer on dataset (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:02<00:10, 11749.34 examples/s]Running tokenizer on dataset (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:09, 12233.58 examples/s]Running tokenizer on dataset (num_proc=1):  21%|â–ˆâ–ˆâ–       | 31000/144715 [00:03<00:09, 12511.24 examples/s]Running tokenizer on dataset (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:03<00:11, 9946.58 examples/s] Running tokenizer on dataset (num_proc=1):  24%|â–ˆâ–ˆâ–       | 35000/144715 [00:03<00:10, 10813.53 examples/s]Running tokenizer on dataset (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:03<00:09, 11496.49 examples/s]Running tokenizer on dataset (num_proc=1):  27%|â–ˆâ–ˆâ–‹       | 39000/144715 [00:03<00:08, 12088.84 examples/s]Running tokenizer on dataset (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 41000/144715 [00:03<00:08, 12579.53 examples/s]Running tokenizer on dataset (num_proc=1):  30%|â–ˆâ–ˆâ–‰       | 43000/144715 [00:04<00:10, 10041.40 examples/s]Running tokenizer on dataset (num_proc=1):  31%|â–ˆâ–ˆâ–ˆ       | 45000/144715 [00:04<00:09, 10977.94 examples/s]Running tokenizer on dataset (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 47000/144715 [00:04<00:08, 11811.74 examples/s]Running tokenizer on dataset (num_proc=1):  34%|â–ˆâ–ˆâ–ˆâ–      | 49000/144715 [00:04<00:07, 12348.03 examples/s]Running tokenizer on dataset (num_proc=1):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 51000/144715 [00:04<00:07, 12667.86 examples/s]Running tokenizer on dataset (num_proc=1):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 53000/144715 [00:04<00:07, 13075.83 examples/s]Running tokenizer on dataset (num_proc=1):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 55000/144715 [00:05<00:08, 10818.46 examples/s]Running tokenizer on dataset (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 57000/144715 [00:05<00:07, 11770.04 examples/s]Running tokenizer on dataset (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:05<00:06, 12704.28 examples/s]Running tokenizer on dataset (num_proc=1):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 61000/144715 [00:05<00:06, 13450.34 examples/s]Running tokenizer on dataset (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:05<00:05, 13811.04 examples/s]Running tokenizer on dataset (num_proc=1):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65000/144715 [00:05<00:07, 11123.29 examples/s]Running tokenizer on dataset (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:06<00:06, 12071.10 examples/s]Running tokenizer on dataset (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69000/144715 [00:06<00:06, 12576.79 examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:06<00:05, 12889.78 examples/s]Running tokenizer on dataset (num_proc=1):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73000/144715 [00:06<00:05, 12934.86 examples/s]Running tokenizer on dataset (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:06<00:06, 10251.95 examples/s]Running tokenizer on dataset (num_proc=1):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 77000/144715 [00:07<00:06, 11067.24 examples/s]Running tokenizer on dataset (num_proc=1):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79000/144715 [00:07<00:05, 11770.59 examples/s]Running tokenizer on dataset (num_proc=1):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 81000/144715 [00:07<00:05, 12206.72 examples/s]Running tokenizer on dataset (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 83000/144715 [00:07<00:04, 12728.08 examples/s]Running tokenizer on dataset (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 85000/144715 [00:07<00:05, 10155.93 examples/s]Running tokenizer on dataset (num_proc=1):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 87000/144715 [00:07<00:05, 11100.74 examples/s]Running tokenizer on dataset (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:08<00:04, 11756.68 examples/s]Running tokenizer on dataset (num_proc=1):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 91000/144715 [00:08<00:04, 12136.49 examples/s]Running tokenizer on dataset (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:08<00:04, 12406.98 examples/s]Running tokenizer on dataset (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 95000/144715 [00:08<00:03, 12642.74 examples/s]Running tokenizer on dataset (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:08<00:04, 9780.61 examples/s] Running tokenizer on dataset (num_proc=1):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 99000/144715 [00:08<00:04, 10656.29 examples/s]Running tokenizer on dataset (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:09<00:03, 11319.73 examples/s]Running tokenizer on dataset (num_proc=1):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 103000/144715 [00:09<00:03, 11860.60 examples/s]Running tokenizer on dataset (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:09<00:03, 12191.07 examples/s]Running tokenizer on dataset (num_proc=1):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107000/144715 [00:09<00:03, 9712.85 examples/s] Running tokenizer on dataset (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:09<00:03, 10554.57 examples/s]Running tokenizer on dataset (num_proc=1):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 111000/144715 [00:10<00:03, 11191.28 examples/s]Running tokenizer on dataset (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:10<00:02, 11770.12 examples/s]Running tokenizer on dataset (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 115000/144715 [00:10<00:02, 12251.38 examples/s]Running tokenizer on dataset (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:10<00:02, 12592.59 examples/s]Running tokenizer on dataset (num_proc=1):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 119000/144715 [00:10<00:02, 9840.57 examples/s] Running tokenizer on dataset (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:10<00:02, 10748.32 examples/s]Running tokenizer on dataset (num_proc=1):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123000/144715 [00:11<00:01, 11460.25 examples/s]Running tokenizer on dataset (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:11<00:01, 12031.75 examples/s]Running tokenizer on dataset (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 127000/144715 [00:11<00:01, 12316.65 examples/s]Running tokenizer on dataset (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:11<00:01, 9823.72 examples/s] Running tokenizer on dataset (num_proc=1):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 131000/144715 [00:11<00:01, 10662.40 examples/s]Running tokenizer on dataset (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:11<00:01, 11264.08 examples/s]Running tokenizer on dataset (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 135000/144715 [00:12<00:00, 11876.27 examples/s]Running tokenizer on dataset (num_proc=1):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137000/144715 [00:12<00:00, 12301.67 examples/s]Running tokenizer on dataset (num_proc=1):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 139000/144715 [00:12<00:00, 9723.76 examples/s] Running tokenizer on dataset (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 141000/144715 [00:12<00:00, 10615.53 examples/s]Running tokenizer on dataset (num_proc=1):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 143000/144715 [00:12<00:00, 11310.77 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:12<00:00, 11791.15 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:14<00:00, 10115.70 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 144715
})
### tokenized_datasets...example [101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102]
RAM used: 1188.20 MB
merge and mask (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]merge and mask (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:25, 5549.17 examples/s]merge and mask (num_proc=1):   6%|â–Œ         | 9000/144715 [00:00<00:08, 16534.23 examples/s]merge and mask (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:00<00:05, 25839.47 examples/s]merge and mask (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:00<00:03, 32961.50 examples/s]merge and mask (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:00<00:03, 38887.40 examples/s]merge and mask (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:01<00:02, 43265.31 examples/s]merge and mask (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:01<00:03, 30847.62 examples/s]merge and mask (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 46000/144715 [00:01<00:02, 35476.22 examples/s]merge and mask (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:01<00:02, 39287.38 examples/s]merge and mask (num_proc=1):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58000/144715 [00:01<00:02, 42981.81 examples/s]merge and mask (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64000/144715 [00:01<00:01, 45971.41 examples/s]merge and mask (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 70000/144715 [00:01<00:01, 48225.65 examples/s]merge and mask (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:02<00:01, 36026.90 examples/s]merge and mask (num_proc=1):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84000/144715 [00:02<00:01, 39878.43 examples/s]merge and mask (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 90000/144715 [00:02<00:01, 43200.06 examples/s]merge and mask (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96000/144715 [00:02<00:01, 45733.14 examples/s]merge and mask (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 102000/144715 [00:02<00:00, 47976.00 examples/s]merge and mask (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108000/144715 [00:02<00:00, 49842.06 examples/s]merge and mask (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114000/144715 [00:02<00:00, 50375.14 examples/s]merge and mask (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122000/144715 [00:03<00:00, 37985.46 examples/s]merge and mask (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 128000/144715 [00:03<00:00, 40770.16 examples/s]merge and mask (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134000/144715 [00:03<00:00, 43135.11 examples/s]merge and mask (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:03<00:00, 44346.55 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:05<00:00, 26843.79 examples/s]
RAM used: 1267.77 MB
padding (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1104927.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476867.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1207689.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1451316.26it/s]
padding (num_proc=1):   1%|â–         | 2000/144715 [00:00<00:55, 2586.40 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1204221.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1463980.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1230723.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1493166.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1251284.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1499572.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1224971.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1505493.18it/s]
padding (num_proc=1):   4%|â–         | 6000/144715 [00:00<00:18, 7484.85 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1204913.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1514736.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1231084.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1489983.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1101156.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1500645.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1224971.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497965.71it/s]
padding (num_proc=1):   7%|â–‹         | 10000/144715 [00:01<00:11, 11377.26 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1242756.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1491573.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1239817.91it/s]

0it [00:00, ?it/s][A1000it [00:00, 1438869.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1200774.12it/s]

0it [00:00, ?it/s][A1000it [00:00, 1443822.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1242756.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1482610.11it/s]
padding (num_proc=1):  10%|â–‰         | 14000/144715 [00:01<00:09, 14483.90 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1238353.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1507116.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1224971.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1509828.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1281486.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460412.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1242020.73it/s]

0it [00:00, ?it/s][A1000it [00:00, 1572077.96it/s]
padding (num_proc=1):  12%|â–ˆâ–        | 18000/144715 [00:01<00:07, 16899.53 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1187515.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1504953.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1231807.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1489454.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1204913.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1165084.44it/s]
padding (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:01<00:09, 12997.00 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1141307.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1489983.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1203876.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1501720.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1181161.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1474790.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1205952.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1453327.79it/s]
padding (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:02<00:07, 15226.94 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1211876.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1462959.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1175533.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476347.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218212.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1469622.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1170939.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475309.18it/s]
padding (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:06, 17013.85 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1159929.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1471169.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1176852.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1469108.23it/s]

0it [00:00, ?it/s][A1000it [00:00, 1220338.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1456861.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1191225.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448809.67it/s]
padding (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:02<00:06, 18446.08 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1218212.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461429.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1198030.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1485235.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1191902.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465003.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1097985.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481562.70it/s]
padding (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:02<00:05, 19610.74 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1212927.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1378345.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1184831.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1438375.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1147238.51it/s]

0it [00:00, ?it/s][A1000it [00:00, 1455344.90it/s]
padding (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:02<00:05, 18532.92 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1190887.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1472202.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1173560.16it/s]

0it [00:00, ?it/s][A1000it [00:00, 1456861.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1188861.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1494762.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1233981.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1462449.09it/s]
padding (num_proc=1):  30%|â–ˆâ–ˆâ–ˆ       | 44000/144715 [00:02<00:05, 19541.66 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1102313.80it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476347.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1221049.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1504413.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1249048.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1512551.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1203530.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1488397.44it/s]
padding (num_proc=1):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 48000/144715 [00:03<00:04, 20491.95 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1113138.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1424695.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1217504.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1487341.84it/s]

0it [00:00, ?it/s][A1000it [00:00, 1216092.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1491043.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1244600.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1495295.54it/s]
padding (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:03<00:04, 21100.00 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1222472.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1505493.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1215740.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483134.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 286496.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478429.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1295738.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1558062.41it/s]
padding (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 56000/144715 [00:03<00:04, 21566.57 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1010188.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 975419.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1183494.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1466539.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1239085.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478950.63it/s]
padding (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:03<00:05, 16734.41 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1242756.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1436405.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1236892.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1466027.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1097410.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1449811.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 999119.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1504413.20it/s]
padding (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:03<00:04, 18299.02 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1223185.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1495295.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1228920.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1499036.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1231084.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1363114.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1224614.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481562.70it/s]
padding (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:04<00:03, 19558.70 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1256155.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1485761.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1222829.15it/s]

0it [00:00, ?it/s][A1000it [00:00, 1501720.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1236163.87it/s]

0it [00:00, ?it/s][A1000it [00:00, 1468079.80it/s]

0it [00:00, ?it/s][A1000it [00:00, 1221049.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477908.39it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:04<00:03, 20511.26 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1242388.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1496362.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1208384.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1473236.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1234708.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475828.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1213980.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1426148.93it/s]
padding (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:04<00:03, 21097.06 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1245709.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1470138.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1249792.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1470138.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1203876.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1471169.41it/s]
padding (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:04<00:03, 18042.76 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1169633.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483134.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1231445.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1471685.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1243862.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481039.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1251657.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1463980.45it/s]
padding (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82000/144715 [00:04<00:03, 19264.32 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1230723.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1488925.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1226046.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478429.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1227481.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1454840.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1234708.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1484184.01it/s]
padding (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 86000/144715 [00:05<00:02, 20309.94 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1209430.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481562.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1240918.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1488397.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1250165.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1291349.75it/s]
padding (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:05<00:03, 16297.55 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1181826.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1486287.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1217504.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1484184.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1219274.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1496896.50it/s]

0it [00:00, ?it/s][A1000it [00:00, 1229640.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448309.39it/s]
padding (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:05<00:02, 17985.72 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1205606.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1495295.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1199400.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1447309.87it/s]

0it [00:00, ?it/s][A1000it [00:00, 1221760.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1508742.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1233256.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1453327.79it/s]
padding (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:05<00:02, 19349.44 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1247191.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1472202.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1211876.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1504413.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1239451.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1453327.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1095404.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1369345.09it/s]
padding (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:05<00:02, 20318.39 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1107553.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460412.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1105218.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1451316.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1026757.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1375181.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1156411.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1413651.50it/s]
padding (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:06<00:01, 20785.69 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1051731.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1404656.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1053580.51it/s]

0it [00:00, ?it/s][A1000it [00:00, 1404186.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1117289.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1407957.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1073535.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1351257.73it/s]
padding (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:06<00:01, 21182.35 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1103183.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1350387.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1148495.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1427119.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1129931.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1412699.23it/s]

0it [00:00, ?it/s][A1000it [00:00, 1111368.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1395775.04it/s]
padding (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:06<00:01, 21437.19 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1101734.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1442829.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1087170.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1150069.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1161535.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1345622.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1128411.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1376987.52it/s]
padding (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:06<00:01, 21574.08 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1128411.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448309.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1106676.52it/s]

0it [00:00, ?it/s][A1000it [00:00, 1391607.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1155455.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1369792.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1089995.84it/s]

0it [00:00, ?it/s][A1000it [00:00, 1400435.39it/s]
padding (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:06<00:01, 21749.64 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1101156.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1381523.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1202840.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1398567.52it/s]

0it [00:00, ?it/s][A1000it [00:00, 1071889.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1347784.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1140376.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1399967.96it/s]
padding (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:06<00:00, 21765.35 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1128714.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1371584.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1072437.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1432970.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1121771.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1355187.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1147552.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1089429.61it/s]
padding (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:07<00:00, 21908.80 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1129322.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1443325.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1073261.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1380159.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1160571.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1420353.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1122972.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1342177.28it/s]
padding (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:07<00:00, 22008.86 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1133595.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1431503.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1156411.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1168981.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 993204.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1374730.91it/s]
padding (num_proc=1):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 136000/144715 [00:07<00:00, 16839.38 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1108138.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1377892.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1108138.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1412223.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1137284.16it/s]

0it [00:00, ?it/s][A1000it [00:00, 1431014.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1151016.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1432970.28it/s]
padding (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:07<00:00, 18205.02 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1122371.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1419392.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1159288.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1391145.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1151648.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1387005.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1149754.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1451818.62it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 144000/144715 [00:07<00:00, 19296.69 examples/s]
0it [00:00, ?it/s][A715it [00:00, 1160126.64it/s]

0it [00:00, ?it/s][A715it [00:00, 1455789.98it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:12<00:00, 11855.21 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 144715
}) padded dataset
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102]])
Column([[101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102], [101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102], [101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102], [101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102]])
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102, 102, 101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102, 102, 101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102, 102, 101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 102, 101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102, 102, 101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1762.41 MB
RAM used: 1762.41 MB
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the VALID set...
### Data samples...
 ['Why does he want to have sex with me not her?', 'When/how did you realize were not straight?'] ['Why did he chose me to have sex with?', 'When/how did you realize you were gay/bisexual? Were you in denial?']
RAM used: 1679.63 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2048
})
RAM used: 1636.59 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 1185.63 examples/s]Running tokenizer on dataset (num_proc=1):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2000/2048 [00:02<00:00, 868.22 examples/s] Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:02<00:00, 752.22 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2048
})
### tokenized_datasets...example [101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102]
RAM used: 1639.95 MB
merge and mask (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]merge and mask (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 1884.97 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 3514.28 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:01<00:00, 1792.66 examples/s]
RAM used: 1642.05 MB
padding (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 748849.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1485761.25it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 2596.79 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1185836.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1459903.93it/s]

0it [00:00, ?it/s][A48it [00:00, 680157.41it/s]

0it [00:00, ?it/s][A48it [00:00, 932067.56it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 2554.75 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2048
}) padded dataset
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102]])
Column([[101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102], [101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102], [101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102], [101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102]])
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102, 102, 101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102, 102, 101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102, 102, 101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102, 102, 101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102, 102, 101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1644.95 MB
RAM used: 1644.95 MB
############################## size of vocab 30522
### Creating model and diffusion...
dict_keys(['lr', 'batch_size', 'microbatch', 'learning_steps', 'log_interval', 'save_interval', 'eval_interval', 'ema_rate', 'resume_checkpoint', 'schedule_sampler', 'diffusion_steps', 'noise_schedule', 'timestep_respacing', 'vocab', 'use_plm_init', 'vocab_size', 'config_name', 'notes', 'data_dir', 'dataset', 'checkpoint_path', 'seq_len', 'hidden_t_dim', 'hidden_dim', 'dropout', 'use_fp16', 'fp16_scale_growth', 'seed', 'gradient_clipping', 'weight_decay', 'learn_sigma', 'use_kl', 'predict_xstart', 'rescale_timesteps', 'rescale_learned_sigmas', 'sigma_small', 'emb_scale_factor'])
initializing from pretrained bert...
BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.57.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

### The parameter count is 109380666
### Saving the hyperparameters to diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:07:07/training_args.json
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /data6/seungwoochoi/x_bridging/DiffuSeq/wandb/offline-run-20251119_010759-12ub2oco
### Training...
cuda:0
get_cls_coditioned_embeds
tensor([[ 3.8086e-01,  4.0672e-01, -1.7457e-01,  ..., -9.9000e-01,
         -2.0360e-01, -1.2644e-01],
        [ 4.1116e-01,  9.1304e-01,  1.8367e-01,  ..., -1.0026e+00,
          2.8973e-02,  1.8683e-01],
        [ 4.3756e-01,  4.3361e-01, -4.2647e-02,  ..., -7.8992e-01,
         -7.6932e-02, -2.1221e-02],
        ...,
        [ 3.9911e-01,  9.2286e-01,  1.2555e-01,  ..., -1.0110e+00,
         -7.2794e-02,  2.8369e-01],
        [ 4.8109e-01,  3.9795e-01, -1.8547e-01,  ..., -6.9457e-01,
         -4.7204e-02,  9.8372e-04],
        [ 3.8643e-01,  3.5252e-01, -1.9594e-01,  ..., -7.9170e-01,
         -9.2208e-02, -3.7918e-02]], device='cuda:0',
       grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3965,  0.5580, -0.1279,  ..., -0.9397, -0.1821, -0.1173],
        [ 0.2137,  0.2257,  0.3390,  ..., -0.4104,  0.2219,  0.2838],
        [ 0.4403,  0.4537, -0.1583,  ..., -0.8638, -0.2215, -0.0366],
        ...,
        [ 0.3652,  0.3443, -0.2070,  ..., -0.9166, -0.1729, -0.1229],
        [ 0.5203,  0.8398, -0.0129,  ..., -0.9399, -0.1026,  0.1473],
        [ 0.3186,  0.2450, -0.2170,  ..., -0.6499,  0.3353,  0.1360]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4628,  0.3842, -0.1087,  ..., -0.5845,  0.3153,  0.1590],
        [ 0.4394,  0.3578, -0.1634,  ..., -0.7894, -0.0571,  0.0449],
        [ 0.3287,  0.3598, -0.1546,  ..., -0.9087, -0.1551, -0.0895],
        ...,
        [ 0.3836,  0.3764, -0.1509,  ..., -0.8369, -0.0914, -0.0432],
        [ 0.3432,  0.3415, -0.1310,  ..., -0.8292, -0.1620,  0.0212],
        [ 0.4597,  0.2901, -0.1950,  ..., -0.8033, -0.1390, -0.0449]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3224,  0.2313, -0.2866,  ..., -0.7257,  0.0859,  0.1248],
        [ 0.5165,  0.4128, -0.1070,  ..., -0.5239,  0.2553,  0.1871],
        [ 0.3238,  0.8199,  0.1338,  ..., -0.8816,  0.1385,  0.2747],
        ...,
        [ 0.1863,  0.5538,  0.2200,  ..., -0.7822,  0.2621,  0.2670],
        [ 0.4683,  0.4103, -0.1584,  ..., -0.6235,  0.1705,  0.1911],
        [ 0.4175,  0.3667, -0.0653,  ..., -0.7709, -0.0634,  0.0532]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2771,  0.3062, -0.1189,  ..., -0.7459,  0.1162,  0.0988],
        [ 0.4857,  0.4228, -0.0720,  ..., -0.6516,  0.0280,  0.1182],
        [ 0.5369,  0.8391,  0.0936,  ..., -0.8923, -0.1059,  0.1105],
        ...,
        [ 0.2733,  0.2808, -0.2056,  ..., -0.8998, -0.1564, -0.1002],
        [ 0.4540,  0.4298, -0.0986,  ..., -0.9481, -0.1809, -0.0164],
        [ 0.3561,  0.8845,  0.1526,  ..., -0.9865,  0.0384,  0.1351]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2825,  0.8403,  0.1011,  ..., -0.9036,  0.0030,  0.2664],
        [ 0.4756,  0.3703, -0.1742,  ..., -0.5720,  0.2335,  0.1717],
        [ 0.5766,  0.4124, -0.1846,  ..., -0.4559,  0.2622,  0.1752],
        ...,
        [ 0.3909,  0.3083, -0.2078,  ..., -0.7683,  0.0665,  0.0672],
        [ 0.4665,  0.3635, -0.1339,  ..., -0.6716,  0.1162,  0.1269],
        [ 0.2679,  0.7479,  0.1588,  ..., -0.8007,  0.0779,  0.2945]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4762,  0.1886,  0.3930,  ..., -0.4562,  0.2433,  0.2282],
        [ 0.4639,  0.3758, -0.1928,  ..., -0.7317, -0.0302,  0.0393],
        [ 0.2907,  0.8121,  0.0688,  ..., -0.9218,  0.0877,  0.2745],
        ...,
        [ 0.2996,  0.2843, -0.1080,  ..., -0.7076,  0.2368,  0.1580],
        [ 0.3643,  0.3353, -0.2601,  ..., -0.7763, -0.0946,  0.0523],
        [ 0.3449,  0.3225, -0.1159,  ..., -0.7242,  0.1568,  0.0932]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3716,  0.9042,  0.1161,  ..., -0.9712, -0.0540,  0.1792],
        [ 0.4074,  0.3348, -0.1626,  ..., -0.7875, -0.0416,  0.0348],
        [ 0.4499,  0.4384, -0.2092,  ..., -0.9542, -0.2685, -0.1468],
        ...,
        [ 0.4116,  0.3542, -0.1411,  ..., -0.8034, -0.0853, -0.0171],
        [ 0.3587,  0.3382, -0.0525,  ..., -0.7574,  0.0634,  0.0673],
        [ 0.4513,  0.3494, -0.1876,  ..., -0.6650,  0.0381,  0.0788]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3972,  0.8151,  0.0727,  ..., -0.9690, -0.1762,  0.1408],
        [ 0.3481,  0.4990, -0.1883,  ..., -0.9385, -0.1827, -0.1277],
        [ 0.5582,  0.8310, -0.0027,  ..., -0.9673, -0.1742,  0.1594],
        ...,
        [ 0.2921,  0.3052, -0.1277,  ..., -0.7889,  0.0886,  0.0648],
        [ 0.5111,  0.7789, -0.0440,  ..., -0.9334, -0.1818,  0.0979],
        [ 0.4387,  0.3654, -0.1649,  ..., -0.7244, -0.0393,  0.0639]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3339,  0.5190, -0.1206,  ..., -0.9381, -0.1506, -0.0044],
        [ 0.3467,  0.2734, -0.1765,  ..., -0.7149,  0.1909,  0.1215],
        [ 0.4911,  0.3988, -0.2390,  ..., -0.8179, -0.1819, -0.0763],
        ...,
        [ 0.4220,  0.3573, -0.1701,  ..., -0.7821, -0.1204, -0.0242],
        [ 0.4615,  0.3608, -0.1584,  ..., -0.5710,  0.2464,  0.1495],
        [ 0.3785,  0.8684, -0.0041,  ..., -0.9313, -0.0479,  0.1154]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4428,  0.4304, -0.2336,  ..., -0.8395, -0.1940, -0.0458],
        [ 0.2583,  0.7103,  0.0799,  ..., -0.9038,  0.1036,  0.2583],
        [ 0.3359,  0.2798, -0.1920,  ..., -0.7090,  0.0709,  0.1261],
        ...,
        [ 0.4218,  0.3670, -0.1854,  ..., -0.7801, -0.0742,  0.0249],
        [ 0.2427,  0.8088,  0.1950,  ..., -0.8791,  0.0925,  0.2905],
        [ 0.2999,  0.3130, -0.1415,  ..., -0.8050,  0.0365,  0.0498]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4516,  0.4095, -0.2054,  ..., -0.8531, -0.2155, -0.0800],
        [ 0.3522,  0.1500,  0.3580,  ..., -0.4049,  0.2527,  0.1954],
        [ 0.3033,  0.3490, -0.0249,  ..., -0.7988,  0.0076,  0.0334],
        ...,
        [ 0.1440,  0.4402,  0.4095,  ..., -0.6266,  0.2682,  0.1892],
        [ 0.3548,  0.4846, -0.1289,  ..., -0.9289, -0.1846, -0.1106],
        [ 0.3149,  0.3268, -0.1366,  ..., -0.7166,  0.2267,  0.1623]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4848,  0.4379, -0.1373,  ..., -0.6960,  0.0076,  0.0597],
        [ 0.4427,  0.3114, -0.2071,  ..., -0.7020,  0.0392,  0.0977],
        [ 0.2877,  0.4203, -0.1379,  ..., -0.9139, -0.1672, -0.1364],
        ...,
        [ 0.3591,  0.4630, -0.1035,  ..., -0.9103, -0.1736, -0.0777],
        [ 0.3397,  0.2991, -0.1249,  ..., -0.7022,  0.2075,  0.1484],
        [ 0.4585,  0.3277, -0.2214,  ..., -0.6437,  0.0405,  0.1365]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.5202,  0.9379,  0.0388,  ..., -0.8929, -0.0668,  0.1876],
        [ 0.4762,  0.5303, -0.1847,  ..., -0.9268, -0.2296, -0.0348],
        [ 0.3537,  0.3608, -0.1586,  ..., -0.7967, -0.0834, -0.0208],
        ...,
        [ 0.4553,  0.9060,  0.1185,  ..., -0.9338,  0.0251,  0.1750],
        [ 0.5018,  0.4016, -0.1642,  ..., -0.6575,  0.0111,  0.0629],
        [ 0.3809,  0.4761, -0.1525,  ..., -0.9729, -0.2086, -0.0857]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4423,  0.3925, -0.1069,  ..., -0.6445,  0.2731,  0.1440],
        [ 0.4551,  0.4350, -0.1927,  ..., -0.8063, -0.2063, -0.0337],
        [ 0.3689,  0.3859, -0.1445,  ..., -0.9072, -0.1541, -0.0566],
        ...,
        [ 0.4651,  0.4164, -0.2079,  ..., -0.6895,  0.0350,  0.0864],
        [ 0.4379,  0.8196,  0.0197,  ..., -0.9390, -0.0289,  0.1782],
        [ 0.3517,  0.4333, -0.1585,  ..., -0.8902, -0.1275, -0.0453]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4132,  0.8681,  0.1296,  ..., -0.9281,  0.0774,  0.1888],
        [ 0.4004,  0.4877, -0.1739,  ..., -0.9806, -0.2320, -0.1420],
        [ 0.2810,  0.8060,  0.1298,  ..., -0.9568,  0.0440,  0.3669],
        ...,
        [ 0.4609,  0.7659,  0.0432,  ..., -0.8985, -0.1481,  0.1126],
        [ 0.3464,  0.8946,  0.0734,  ..., -0.9649,  0.0392,  0.2553],
        [ 0.3901,  0.3056, -0.1205,  ..., -0.6271,  0.3094,  0.1631]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
------------------------
| grad_norm | 70.1     |
| loss      | 10.1     |
| loss_q0   | 10.1     |
| loss_q1   | 10.1     |
| loss_q2   | 10.1     |
| loss_q3   | 10.1     |
| mse       | 0.256    |
| mse_q0    | 0.268    |
| mse_q1    | 0.264    |
| mse_q2    | 0.246    |
| mse_q3    | 0.247    |
| nll       | 11       |
| nll_q0    | 11       |
| nll_q1    | 11       |
| nll_q2    | 11       |
| nll_q3    | 11       |
| samples   | 1.02e+03 |
| step      | 0        |
------------------------
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
get_cls_coditioned_embeds
tensor([[ 0.3468,  0.5542,  0.3732,  ..., -0.4739,  0.1178,  0.0671],
        [ 0.2398,  0.3477,  0.3485,  ..., -0.2523,  0.1748,  0.2296],
        [ 0.2489,  0.2749,  0.3955,  ..., -0.2203,  0.2397,  0.2297],
        ...,
        [ 0.2908,  0.4513,  0.4815,  ..., -0.2203,  0.2023,  0.1264],
        [ 0.2898,  0.4778,  0.5367,  ..., -0.3759,  0.1331,  0.0808],
        [ 0.2735,  0.0653,  0.4368,  ..., -0.0716,  0.1101,  0.1704]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3792,  0.4988,  0.5297,  ..., -0.4392,  0.0644,  0.0287],
        [ 0.2845,  0.5487,  0.3681,  ..., -0.4457,  0.1397,  0.1784],
        [ 0.2125,  0.2914,  0.3634,  ..., -0.2247,  0.2365,  0.1982],
        ...,
        [ 0.3414,  0.4677,  0.4897,  ..., -0.2561,  0.1267,  0.0845],
        [ 0.2972,  0.5556,  0.4131,  ..., -0.4754,  0.1413,  0.1186],
        [ 0.3524,  0.5142,  0.4949,  ..., -0.2911,  0.1208,  0.0893]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2386,  0.4827,  0.3394,  ..., -0.4564,  0.1542,  0.1949],
        [ 0.3743,  0.4906,  0.3672,  ..., -0.4318,  0.0900,  0.0610],
        [ 0.2836,  0.4714,  0.3133,  ..., -0.3910,  0.1119,  0.1882],
        ...,
        [ 0.2204,  0.2281,  0.4071,  ..., -0.2005,  0.2234,  0.2896],
        [ 0.3868,  0.4624,  0.4729,  ..., -0.4171,  0.0610,  0.0067],
        [ 0.2976,  0.5249,  0.5274,  ..., -0.2657,  0.1476,  0.1360]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2242,  0.4799,  0.3408,  ..., -0.3776,  0.1533,  0.1949],
        [ 0.3032,  0.4853,  0.5208,  ..., -0.3653,  0.1121,  0.0567],
        [ 0.3333,  0.5441,  0.3900,  ..., -0.4567,  0.1289,  0.1380],
        ...,
        [ 0.4044,  0.4688,  0.4883,  ..., -0.3733,  0.0817,  0.0353],
        [ 0.1931,  0.4881,  0.3804,  ..., -0.4378,  0.1576,  0.1670],
        [ 0.3302,  0.5138,  0.3825,  ..., -0.4349,  0.1236,  0.1736]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3991,  0.4763,  0.4695,  ..., -0.3951,  0.0633, -0.0126],
        [ 0.4061,  0.4662,  0.5383,  ..., -0.4352,  0.0629,  0.0302],
        [ 0.2912,  0.3648,  0.3964,  ..., -0.1625,  0.2295,  0.1313],
        ...,
        [ 0.3589,  0.4918,  0.5039,  ..., -0.2799,  0.1204,  0.0714],
        [ 0.4088,  0.5103,  0.5087,  ..., -0.4701,  0.0479,  0.0671],
        [ 0.3715,  0.4964,  0.5233,  ..., -0.4834,  0.0966,  0.0228]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2977,  0.4174,  0.4586,  ..., -0.1883,  0.1823,  0.1151],
        [ 0.2852,  0.1104,  0.3614,  ..., -0.0793,  0.1389,  0.1942],
        [ 0.3081,  0.4982,  0.4828,  ..., -0.3909,  0.0862,  0.0665],
        ...,
        [ 0.4112,  0.4984,  0.4879,  ..., -0.4284,  0.0677,  0.0379],
        [ 0.2363,  0.5015,  0.3540,  ..., -0.4508,  0.1627,  0.2095],
        [ 0.2346,  0.3740,  0.3900,  ..., -0.2572,  0.1786,  0.2424]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3845,  0.4722,  0.5343,  ..., -0.4596,  0.1065,  0.0064],
        [ 0.2553,  0.4049,  0.4004,  ..., -0.1710,  0.1938,  0.1373],
        [ 0.2795,  0.4908,  0.3645,  ..., -0.4483,  0.1528,  0.2045],
        ...,
        [ 0.3340,  0.5137,  0.3763,  ..., -0.4294,  0.1218,  0.0840],
        [ 0.3891,  0.4926,  0.4367,  ..., -0.4800,  0.0531, -0.0107],
        [ 0.1845,  0.3763,  0.3444,  ..., -0.2472,  0.1881,  0.1789]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2060,  0.4677,  0.3076,  ..., -0.4101,  0.1582,  0.2222],
        [ 0.3951,  0.5049,  0.5023,  ..., -0.3958,  0.0682,  0.0349],
        [ 0.2139,  0.1805,  0.4169,  ..., -0.0722,  0.1717,  0.2280],
        ...,
        [ 0.3331,  0.4882,  0.5313,  ..., -0.3047,  0.0966,  0.0658],
        [ 0.3144,  0.5776,  0.3722,  ..., -0.5004,  0.1086,  0.1708],
        [ 0.1855,  0.1349,  0.4358,  ..., -0.1099,  0.1798,  0.2098]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2735,  0.1190,  0.3618,  ..., -0.0591,  0.1231,  0.1875],
        [ 0.3216,  0.5551,  0.3802,  ..., -0.4814,  0.1077,  0.1816],
        [ 0.1612,  0.3458,  0.3798,  ..., -0.2287,  0.1875,  0.2364],
        ...,
        [ 0.3859,  0.4786,  0.4994,  ..., -0.3529,  0.0802,  0.0336],
        [ 0.2771,  0.5257,  0.3834,  ..., -0.4435,  0.1518,  0.1733],
        [ 0.3682,  0.5249,  0.4159,  ..., -0.4897,  0.1106,  0.0817]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3062,  0.4670,  0.4936,  ..., -0.2731,  0.1357,  0.1540],
        [ 0.2756,  0.4586,  0.4888,  ..., -0.2448,  0.1444,  0.1556],
        [ 0.3867,  0.5173,  0.3955,  ..., -0.4905,  0.1036,  0.0564],
        ...,
        [ 0.4066,  0.5259,  0.4826,  ..., -0.4331,  0.0456,  0.0669],
        [ 0.2150,  0.2472,  0.3492,  ..., -0.1449,  0.1732,  0.2315],
        [ 0.4038,  0.4699,  0.4113,  ..., -0.4361,  0.0831,  0.0454]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3449,  0.5076,  0.5076,  ..., -0.3456,  0.0972,  0.0680],
        [ 0.2885,  0.3799,  0.3960,  ..., -0.1594,  0.1965,  0.1190],
        [ 0.2139,  0.5245,  0.3701,  ..., -0.4591,  0.1648,  0.1475],
        ...,
        [ 0.3335,  0.4850,  0.5211,  ..., -0.3530,  0.1200,  0.0709],
        [ 0.3789,  0.4883,  0.4937,  ..., -0.4437,  0.0714,  0.0278],
        [ 0.3183,  0.5305,  0.3846,  ..., -0.5016,  0.1112,  0.1074]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3519,  0.4741,  0.3605,  ..., -0.4337,  0.0761,  0.0106],
        [ 0.4337,  0.4357,  0.4452,  ..., -0.4499,  0.0482, -0.0073],
        [ 0.3361,  0.5042,  0.3738,  ..., -0.4458,  0.1387,  0.1044],
        ...,
        [ 0.3823,  0.4371,  0.4432,  ..., -0.4521,  0.0690,  0.0037],
        [ 0.3195,  0.5943,  0.4041,  ..., -0.4671,  0.1233,  0.1225],
        [ 0.3312,  0.4662,  0.4875,  ..., -0.2379,  0.1354,  0.0845]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.1946,  0.3498,  0.4156,  ..., -0.2705,  0.2014,  0.2352],
        [ 0.3393,  0.5559,  0.3611,  ..., -0.4996,  0.1222,  0.1366],
        [ 0.2638,  0.3730,  0.4193,  ..., -0.2005,  0.2355,  0.1448],
        ...,
        [ 0.3218,  0.5332,  0.3461,  ..., -0.4682,  0.1005,  0.1692],
        [ 0.3795,  0.5143,  0.4212,  ..., -0.4686,  0.1150,  0.0727],
        [ 0.2426,  0.3789,  0.3318,  ..., -0.2825,  0.1839,  0.2270]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4723,  0.1562,  0.3505,  ..., -0.0779,  0.0566,  0.1311],
        [ 0.4250,  0.4753,  0.5033,  ..., -0.2852,  0.0731,  0.0497],
        [ 0.3409,  0.5480,  0.4065,  ..., -0.5084,  0.1131,  0.1050],
        ...,
        [ 0.3832,  0.4697,  0.4154,  ..., -0.4529,  0.0969,  0.1117],
        [ 0.3860,  0.4935,  0.5096,  ..., -0.4557,  0.0891,  0.0040],
        [ 0.2858,  0.3953,  0.4372,  ..., -0.1894,  0.2199,  0.1399]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3682,  0.4985,  0.5015,  ..., -0.3398,  0.0941,  0.0348],
        [ 0.4054,  0.5028,  0.5103,  ..., -0.3332,  0.0796,  0.0610],
        [ 0.4385,  0.4702,  0.5014,  ..., -0.4175,  0.0678, -0.0051],
        ...,
        [ 0.3480,  0.4918,  0.4824,  ..., -0.2726,  0.1183,  0.1075],
        [ 0.3406,  0.4451,  0.4030,  ..., -0.4370,  0.0931,  0.0270],
        [ 0.4722,  0.4814,  0.4264,  ..., -0.4468,  0.0369, -0.0370]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2897,  0.5430,  0.3654,  ..., -0.4918,  0.1253,  0.1260],
        [ 0.4016,  0.4858,  0.4899,  ..., -0.4064,  0.0881,  0.0039],
        [ 0.3211,  0.5444,  0.3989,  ..., -0.4965,  0.1134,  0.0933],
        ...,
        [ 0.3421,  0.5014,  0.5101,  ..., -0.2956,  0.1297,  0.0918],
        [ 0.3421,  0.4718,  0.4911,  ..., -0.3105,  0.1070,  0.0830],
        [ 0.3164,  0.5323,  0.3432,  ..., -0.4450,  0.0970,  0.1981]],
       device='cuda:0')
x_start_mean::::
torch.Size([64, 128, 768])
eval on validation set
---------------------------
| eval_loss    | 10       |
| eval_loss_q0 | 10       |
| eval_loss_q1 | 10       |
| eval_loss_q2 | 10       |
| eval_loss_q3 | 10       |
| eval_mse     | 0.142    |
| eval_mse_q0  | 0.141    |
| eval_mse_q1  | 0.14     |
| eval_mse_q2  | 0.142    |
| eval_mse_q3  | 0.143    |
| eval_nll     | 10.5     |
| eval_nll_q0  | 10.5     |
| eval_nll_q1  | 10.5     |
| eval_nll_q2  | 10.5     |
| eval_nll_q3  | 10.6     |
---------------------------
get_cls_coditioned_embeds
tensor([[ 0.2078,  0.5132,  0.3529,  ..., -0.4735,  0.1593,  0.1915],
        [ 0.1926,  0.3985,  0.3922,  ..., -0.3541,  0.1965,  0.2163],
        [ 0.1877,  0.4216,  0.3564,  ..., -0.2772,  0.1697,  0.2203],
        ...,
        [ 0.2521,  0.3781,  0.3474,  ..., -0.2824,  0.1790,  0.2219],
        [ 0.2708,  0.4334,  0.4576,  ..., -0.2115,  0.1997,  0.1369],
        [ 0.2945,  0.3908,  0.3995,  ..., -0.1515,  0.2093,  0.1634]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3620,  0.4932,  0.3856,  ..., -0.4163,  0.0998,  0.0863],
        [ 0.3225,  0.4890,  0.5255,  ..., -0.2396,  0.1248,  0.1074],
        [ 0.3771,  0.5147,  0.4689,  ..., -0.3315,  0.0706,  0.0380],
        ...,
        [ 0.3946,  0.4853,  0.4953,  ..., -0.2181,  0.1281,  0.0832],
        [ 0.3610,  0.5501,  0.3609,  ..., -0.4577,  0.1068,  0.1560],
        [ 0.1721,  0.3988,  0.3687,  ..., -0.2927,  0.1880,  0.2419]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3260,  0.0816,  0.2701,  ..., -0.1184,  0.0759,  0.1320],
        [ 0.3229,  0.3788,  0.4104,  ..., -0.1760,  0.2082,  0.1311],
        [ 0.4946,  0.5004,  0.4646,  ..., -0.4432,  0.0522,  0.0098],
        ...,
        [ 0.2188,  0.4211,  0.3678,  ..., -0.3370,  0.1925,  0.2016],
        [ 0.3738,  0.5168,  0.5053,  ..., -0.3642,  0.0842,  0.0413],
        [ 0.2889,  0.4861,  0.3315,  ..., -0.4273,  0.1014,  0.1968]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3605,  0.4006,  0.3863,  ..., -0.2309,  0.0973,  0.0181],
        [ 0.3886,  0.5070,  0.4997,  ..., -0.4535,  0.0847,  0.0043],
        [ 0.3536,  0.4698,  0.4833,  ..., -0.4234,  0.0697,  0.0247],
        ...,
        [ 0.3672,  0.4831,  0.4958,  ..., -0.2805,  0.1029,  0.0687],
        [ 0.3239,  0.5081,  0.4032,  ..., -0.4479,  0.1247,  0.0813],
        [ 0.3033,  0.0795,  0.4209,  ..., -0.0688,  0.1008,  0.1900]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3416,  0.4865,  0.5084,  ..., -0.3958,  0.0838,  0.0157],
        [ 0.2225,  0.1888,  0.4258,  ..., -0.1246,  0.1734,  0.2151],
        [ 0.3207,  0.4461,  0.4482,  ..., -0.2045,  0.1839,  0.1156],
        ...,
        [ 0.3031,  0.0796,  0.4868,  ...,  0.0047,  0.1070,  0.2333],
        [ 0.4024,  0.4711,  0.5428,  ..., -0.2930,  0.0761,  0.1252],
        [ 0.3492,  0.5398,  0.3816,  ..., -0.4561,  0.0994,  0.0844]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3255,  0.4617,  0.4636,  ..., -0.2139,  0.1578,  0.1109],
        [ 0.2442,  0.3995,  0.4728,  ..., -0.2288,  0.1900,  0.1382],
        [ 0.2302,  0.5063,  0.3079,  ..., -0.4605,  0.1407,  0.1770],
        ...,
        [ 0.3144,  0.0421,  0.4608,  ..., -0.1576,  0.0535,  0.1653],
        [ 0.4070,  0.4833,  0.4711,  ..., -0.3153,  0.0707,  0.0453],
        [ 0.3151,  0.5737,  0.3800,  ..., -0.4945,  0.1298,  0.1274]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3568,  0.5645,  0.4010,  ..., -0.4729,  0.1177,  0.1000],
        [ 0.2574,  0.4428,  0.5113,  ..., -0.2384,  0.1863,  0.1364],
        [ 0.3887,  0.4588,  0.4841,  ..., -0.4492,  0.0662, -0.0276],
        ...,
        [ 0.4111,  0.5291,  0.4985,  ..., -0.3867,  0.0367,  0.0106],
        [ 0.3331,  0.5278,  0.5122,  ..., -0.3255,  0.1351,  0.1272],
        [ 0.3578,  0.5547,  0.4071,  ..., -0.4968,  0.1280,  0.0702]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3892,  0.4668,  0.4805,  ..., -0.3262,  0.0828,  0.0293],
        [ 0.4254,  0.4808,  0.4756,  ..., -0.3843,  0.0583, -0.0069],
        [ 0.2137,  0.4676,  0.3592,  ..., -0.4283,  0.1390,  0.2338],
        ...,
        [ 0.3492,  0.3909,  0.4184,  ..., -0.1712,  0.2000,  0.1216],
        [ 0.3742,  0.5356,  0.3907,  ..., -0.4512,  0.1028,  0.0942],
        [ 0.4231,  0.5143,  0.4839,  ..., -0.2660,  0.0766,  0.0444]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3570,  0.5555,  0.3805,  ..., -0.4649,  0.1071,  0.0789],
        [ 0.2847,  0.5225,  0.3864,  ..., -0.4843,  0.1357,  0.1742],
        [ 0.2529,  0.1096,  0.3510,  ..., -0.1394,  0.1481,  0.1800],
        ...,
        [ 0.4283,  0.4775,  0.5230,  ..., -0.4286,  0.0500,  0.0104],
        [ 0.3075,  0.3954,  0.4127,  ..., -0.1719,  0.2215,  0.1362],
        [ 0.2672,  0.5198,  0.3501,  ..., -0.4359,  0.1280,  0.1482]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2342,  0.5011,  0.3144,  ..., -0.4259,  0.1215,  0.2018],
        [ 0.3208,  0.4915,  0.5033,  ..., -0.2966,  0.1262,  0.0948],
        [ 0.2156,  0.3704,  0.3489,  ..., -0.2635,  0.1961,  0.2142],
        ...,
        [ 0.4399,  0.4641,  0.4379,  ..., -0.4413,  0.0461, -0.0133],
        [ 0.3272,  0.4954,  0.4270,  ..., -0.4624,  0.1124,  0.0608],
        [ 0.2557,  0.1124,  0.4475,  ..., -0.0550,  0.1290,  0.1915]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3845,  0.4779,  0.5111,  ..., -0.2986,  0.0813,  0.0529],
        [ 0.3749,  0.4515,  0.4534,  ..., -0.1943,  0.1298,  0.0937],
        [ 0.3750,  0.4962,  0.5139,  ..., -0.3543,  0.0937,  0.0567],
        ...,
        [ 0.2649,  0.3878,  0.4368,  ..., -0.2004,  0.2313,  0.1438],
        [ 0.2483,  0.4504,  0.3384,  ..., -0.3382,  0.1785,  0.1951],
        [ 0.3838,  0.4881,  0.4066,  ..., -0.4585,  0.0821,  0.0338]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.1820,  0.4105,  0.3100,  ..., -0.3072,  0.1945,  0.2216],
        [ 0.3307,  0.4511,  0.4665,  ..., -0.1874,  0.1740,  0.1672],
        [ 0.2914,  0.4426,  0.4606,  ..., -0.2402,  0.2020,  0.1152],
        ...,
        [ 0.3556,  0.5157,  0.4593,  ..., -0.3957,  0.0844, -0.0042],
        [ 0.3423,  0.4618,  0.5200,  ..., -0.3263,  0.0988,  0.0775],
        [ 0.2451,  0.5426,  0.3783,  ..., -0.4763,  0.1548,  0.1974]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4076,  0.5291,  0.3893,  ..., -0.4476,  0.0924,  0.0898],
        [ 0.3771,  0.5116,  0.4854,  ..., -0.3533,  0.0857,  0.0283],
        [ 0.3532,  0.5287,  0.3983,  ..., -0.4440,  0.1093,  0.0602],
        ...,
        [ 0.3787,  0.5131,  0.5130,  ..., -0.4379,  0.0637, -0.0014],
        [ 0.3740,  0.4995,  0.4244,  ..., -0.4738,  0.0897,  0.0749],
        [ 0.3273,  0.5551,  0.4018,  ..., -0.5003,  0.1240,  0.0852]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4075,  0.5336,  0.3907,  ..., -0.4548,  0.0995,  0.0701],
        [ 0.3217,  0.5535,  0.4100,  ..., -0.4782,  0.1242,  0.0937],
        [ 0.3104,  0.0603,  0.3273,  ..., -0.1255,  0.0817,  0.1928],
        ...,
        [ 0.3599,  0.4672,  0.4120,  ..., -0.4540,  0.0923,  0.0466],
        [ 0.2199,  0.4293,  0.3463,  ..., -0.3466,  0.1453,  0.1666],
        [ 0.3558,  0.4909,  0.4832,  ..., -0.3209,  0.1231,  0.0822]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.4207,  0.4736,  0.4767,  ..., -0.4243,  0.0479, -0.0096],
        [ 0.3533,  0.4751,  0.5183,  ..., -0.3696,  0.0811,  0.0189],
        [ 0.2396,  0.1685,  0.3435,  ..., -0.0932,  0.1973,  0.1814],
        ...,
        [ 0.3088,  0.5582,  0.4003,  ..., -0.4950,  0.1427,  0.1216],
        [ 0.2960,  0.4508,  0.5393,  ..., -0.3785,  0.1341,  0.0817],
        [ 0.3105,  0.4932,  0.4926,  ..., -0.3300,  0.1042,  0.0708]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2881,  0.1250,  0.3858,  ..., -0.0737,  0.1182,  0.2823],
        [ 0.3350,  0.4732,  0.4680,  ..., -0.3006,  0.1213,  0.0429],
        [ 0.3600,  0.5016,  0.5340,  ..., -0.4200,  0.0657,  0.0134],
        ...,
        [ 0.2158,  0.5304,  0.3501,  ..., -0.4507,  0.1611,  0.1660],
        [ 0.3414,  0.4353,  0.5004,  ..., -0.3217,  0.1126,  0.0574],
        [ 0.3756,  0.5370,  0.3907,  ..., -0.4601,  0.1070,  0.1252]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3013,  0.3498,  0.3526,  ..., -0.0020,  0.1754,  0.1502],
        [ 0.2073,  0.0844,  0.3733,  ...,  0.0420,  0.1171,  0.2169],
        [ 0.3948,  0.3874,  0.3548,  ..., -0.1637,  0.1263,  0.0910],
        ...,
        [ 0.3437,  0.0397,  0.3117,  ...,  0.0202,  0.1097,  0.1418],
        [ 0.3587,  0.4063,  0.3728,  ..., -0.0057,  0.1625,  0.1646],
        [ 0.3438,  0.4095,  0.3669,  ...,  0.0456,  0.1455,  0.1472]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3855,  0.4292,  0.3844,  ..., -0.0008,  0.1504,  0.1537],
        [ 0.3145,  0.3941,  0.3294,  ...,  0.0353,  0.1763,  0.1670],
        [ 0.3317,  0.3821,  0.3147,  ...,  0.0294,  0.1570,  0.1403],
        ...,
        [ 0.3130,  0.3768,  0.3009,  ..., -0.2132,  0.1284,  0.1601],
        [ 0.3108,  0.3525,  0.3098,  ..., -0.1610,  0.1303,  0.1596],
        [ 0.2545,  0.3465,  0.2807,  ..., -0.0956,  0.1403,  0.1889]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3648,  0.4160,  0.3564,  ...,  0.0340,  0.1594,  0.1583],
        [ 0.3343,  0.4010,  0.3594,  ...,  0.0169,  0.1697,  0.1732],
        [ 0.3282,  0.4323,  0.4028,  ..., -0.0450,  0.1665,  0.1623],
        ...,
        [ 0.3458,  0.3496,  0.2936,  ..., -0.2128,  0.0804,  0.0941],
        [ 0.3436,  0.0469,  0.2673,  ..., -0.0333,  0.0403,  0.0998],
        [ 0.2357,  0.3314,  0.2762,  ..., -0.1582,  0.1398,  0.1971]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3460,  0.4216,  0.3967,  ..., -0.0330,  0.1697,  0.1380],
        [ 0.3560,  0.4244,  0.4069,  ..., -0.1203,  0.1625,  0.1194],
        [ 0.3379,  0.3999,  0.3637,  ...,  0.0277,  0.1743,  0.1579],
        ...,
        [ 0.2979,  0.4000,  0.3215,  ...,  0.0296,  0.1666,  0.1598],
        [ 0.3345,  0.4052,  0.3758,  ..., -0.0153,  0.1542,  0.1537],
        [ 0.3003,  0.3372,  0.3214,  ...,  0.0200,  0.1522,  0.1415]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3390,  0.3493,  0.3098,  ..., -0.1889,  0.0839,  0.0785],
        [ 0.2398,  0.2940,  0.3021,  ..., -0.1309,  0.1795,  0.1963],
        [ 0.2495,  0.3296,  0.3116,  ..., -0.1396,  0.1453,  0.2240],
        ...,
        [ 0.2687,  0.3174,  0.2936,  ..., -0.1006,  0.1289,  0.1833],
        [ 0.3105,  0.4124,  0.3508,  ...,  0.0228,  0.1650,  0.1687],
        [ 0.3224,  0.4215,  0.4036,  ..., -0.0259,  0.1723,  0.1737]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3172,  0.4065,  0.3757,  ..., -0.0039,  0.1644,  0.1704],
        [ 0.3694,  0.4346,  0.3942,  ..., -0.0562,  0.1580,  0.1403],
        [ 0.2680,  0.0538,  0.3361,  ...,  0.0090,  0.1169,  0.1725],
        ...,
        [ 0.2502,  0.3224,  0.3191,  ..., -0.1321,  0.1472,  0.2268],
        [ 0.3625,  0.4046,  0.4145,  ..., -0.0670,  0.1659,  0.1248],
        [ 0.2015,  0.2491,  0.2921,  ..., -0.1128,  0.1491,  0.1971]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3315,  0.3924,  0.3701,  ...,  0.0149,  0.1613,  0.1689],
        [ 0.2157,  0.3082,  0.3039,  ..., -0.0957,  0.1469,  0.1967],
        [ 0.2825,  0.4125,  0.3606,  ...,  0.0201,  0.1697,  0.2060],
        ...,
        [ 0.2930,  0.3537,  0.3093,  ...,  0.0247,  0.1567,  0.1541],
        [ 0.4141,  0.4095,  0.4169,  ..., -0.1342,  0.1633,  0.1252],
        [ 0.3837,  0.4329,  0.3955,  ..., -0.0320,  0.1550,  0.1546]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3460,  0.3968,  0.3515,  ...,  0.0294,  0.1582,  0.1428],
        [ 0.3245,  0.3507,  0.3020,  ..., -0.2138,  0.0983,  0.0884],
        [ 0.3606,  0.4287,  0.3761,  ...,  0.0212,  0.1579,  0.1732],
        ...,
        [ 0.4317,  0.3950,  0.3830,  ..., -0.1371,  0.1630,  0.1203],
        [ 0.3461,  0.3746,  0.3173,  ..., -0.2059,  0.0851,  0.1496],
        [ 0.3370,  0.4166,  0.3667,  ...,  0.0112,  0.1480,  0.1700]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3853,  0.4144,  0.4087,  ..., -0.0891,  0.1558,  0.1163],
        [ 0.2859,  0.3643,  0.3620,  ...,  0.0263,  0.1457,  0.1586],
        [ 0.3457,  0.3525,  0.3051,  ..., -0.2244,  0.0750,  0.0988],
        ...,
        [ 0.2233,  0.1098,  0.3597,  ...,  0.0350,  0.1253,  0.2032],
        [ 0.3665,  0.4370,  0.3975,  ..., -0.0196,  0.1629,  0.1554],
        [ 0.3543,  0.3924,  0.3900,  ..., -0.0534,  0.1439,  0.1411]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2961,  0.3382,  0.3059,  ..., -0.1297,  0.1332,  0.2146],
        [ 0.2345,  0.1995,  0.3079,  ..., -0.0331,  0.1574,  0.1976],
        [ 0.3339,  0.4097,  0.3919,  ..., -0.0247,  0.1639,  0.1609],
        ...,
        [ 0.3250,  0.4001,  0.3534,  ...,  0.0387,  0.1463,  0.1775],
        [ 0.3717,  0.3575,  0.3280,  ..., -0.1939,  0.0734,  0.0790],
        [ 0.2963,  0.3899,  0.2976,  ..., -0.2049,  0.1232,  0.1862]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2403,  0.1801,  0.3435,  ...,  0.0166,  0.1578,  0.1835],
        [ 0.3461,  0.1938,  0.2464,  ..., -0.0936,  0.0008,  0.0811],
        [ 0.3260,  0.4095,  0.4076,  ..., -0.0968,  0.1801,  0.1218],
        ...,
        [ 0.3450,  0.0302,  0.2622,  ..., -0.0159,  0.0284,  0.0859],
        [ 0.2611,  0.3443,  0.3541,  ...,  0.0205,  0.1748,  0.1836],
        [ 0.2891,  0.2966,  0.3539,  ..., -0.0051,  0.1884,  0.1115]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.3580,  0.3727,  0.3518,  ..., -0.2042,  0.1048,  0.0844],
        [ 0.3548,  0.4135,  0.4007,  ..., -0.0229,  0.1512,  0.1637],
        [ 0.2371,  0.3627,  0.3265,  ..., -0.2087,  0.1414,  0.1911],
        ...,
        [ 0.2350,  0.2923,  0.3074,  ..., -0.1098,  0.1705,  0.2251],
        [ 0.3239,  0.4282,  0.3519,  ...,  0.0109,  0.1628,  0.1573],
        [ 0.4042,  0.4174,  0.4022,  ..., -0.1458,  0.1667,  0.1261]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2333, -0.0091,  0.3386,  ..., -0.0078,  0.0779,  0.1795],
        [ 0.3296,  0.3906,  0.3499,  ...,  0.0367,  0.1582,  0.1464],
        [ 0.3075,  0.3527,  0.2790,  ..., -0.2155,  0.0959,  0.1086],
        ...,
        [ 0.3767,  0.4035,  0.4025,  ..., -0.0197,  0.1611,  0.1447],
        [ 0.3436,  0.3724,  0.3237,  ..., -0.2018,  0.0976,  0.1155],
        [ 0.3051,  0.3944,  0.2999,  ..., -0.2249,  0.1311,  0.1361]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2975,  0.3747,  0.2994,  ..., -0.1882,  0.1122,  0.1829],
        [ 0.3441,  0.3940,  0.4484,  ..., -0.0697,  0.1707,  0.1590],
        [ 0.2320,  0.1456,  0.2922,  ..., -0.0008,  0.1272,  0.1964],
        ...,
        [ 0.3129,  0.3488,  0.3372,  ...,  0.0128,  0.1682,  0.1585],
        [ 0.3049,  0.4178,  0.3725,  ..., -0.0178,  0.1774,  0.1725],
        [ 0.3460,  0.4150,  0.3673,  ...,  0.0202,  0.1620,  0.1682]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2854,  0.3723,  0.3106,  ..., -0.1512,  0.1212,  0.2236],
        [ 0.3253,  0.4053,  0.3536,  ...,  0.0107,  0.1761,  0.1599],
        [ 0.3677,  0.4032,  0.3821,  ..., -0.0739,  0.1683,  0.1282],
        ...,
        [ 0.3801,  0.3977,  0.4049,  ..., -0.0892,  0.1554,  0.1141],
        [ 0.3516,  0.4251,  0.3791,  ..., -0.0264,  0.1446,  0.1553],
        [ 0.3430,  0.4330,  0.3938,  ..., -0.0712,  0.1593,  0.1362]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.2938,  0.3565,  0.3148,  ...,  0.0076,  0.1538,  0.1596],
        [ 0.3276,  0.3697,  0.2973,  ..., -0.2198,  0.0882,  0.1480],
        [ 0.3237,  0.3447,  0.3132,  ..., -0.1951,  0.0993,  0.1431],
        ...,
        [ 0.2980,  0.3694,  0.2975,  ..., -0.1676,  0.1171,  0.1867],
        [ 0.3090,  0.3655,  0.3163,  ..., -0.2331,  0.1120,  0.1395],
        [ 0.3099,  0.1484,  0.3005,  ..., -0.0256,  0.0060,  0.1369]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.1822,  0.3539,  0.1723,  ...,  0.0334,  0.1424,  0.2127],
        [ 0.1829,  0.3540,  0.1562,  ...,  0.0660,  0.1537,  0.2220],
        [ 0.2114,  0.3402,  0.1999,  ...,  0.0161,  0.1557,  0.2042],
        ...,
        [ 0.2107,  0.1603,  0.2528,  ...,  0.0357,  0.0836,  0.1682],
        [ 0.2479,  0.0133,  0.2478,  ...,  0.0614,  0.0186,  0.0961],
        [ 0.2086,  0.3401,  0.2234,  ..., -0.0076,  0.1731,  0.1735]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
get_cls_coditioned_embeds
tensor([[ 0.1926,  0.3512,  0.1441,  ...,  0.0594,  0.1335,  0.2241],
        [ 0.1751,  0.3424,  0.1394,  ...,  0.0493,  0.1412,  0.2190],
        [ 0.2488,  0.3539,  0.2123,  ..., -0.0042,  0.1538,  0.1774],
        ...,
        [ 0.2556,  0.3582,  0.2113,  ..., -0.0169,  0.1586,  0.1774],
        [ 0.2771,  0.3411,  0.2307,  ..., -0.0199,  0.1676,  0.1556],
        [ 0.2288,  0.3324,  0.2284,  ..., -0.0210,  0.1648,  0.1601]],
       device='cuda:0', grad_fn=<SelectBackward0>)
x_start_mean::::
torch.Size([64, 128, 768])
slurmstepd-n02: error: *** JOB 88817 ON n02 CANCELLED AT 2025-11-19T01:08:31 ***
