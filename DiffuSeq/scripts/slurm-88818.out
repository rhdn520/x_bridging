 OPENAI_LOGDIR=diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:10:25  TOKENIZERS_PARALLELISM=false python train.py   --checkpoint_path diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:10:25 --dataset qqp --data_dir /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp --vocab bert --use_plm_init bert --lr 0.0001 --batch_size 1024 --microbatch 64 --diffusion_steps 2000 --noise_schedule sqrt --schedule_sampler lossaware --resume_checkpoint none --seq_len 128 --hidden_t_dim 128 --seed 102 --hidden_dim 768 --learning_steps 50000 --save_interval 10000 --config_name bert-base-uncased --notes test-qqp20251119-01:10:25 
Logging to diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:10:25
### Creating data loader...
initializing the random embeddings Embedding(30522, 768)
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the TRAIN set...
### Data samples...
 ['Academic and Educational Advice: What can I do after completing bcom?', 'What are some good songs to make a texting lyric prank?'] ['What should I do after bcom?', 'What is a good prank lyric text to text to a friend?']
RAM used: 1022.70 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 144715
})
RAM used: 1070.02 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):   1%|          | 1000/144715 [00:00<01:11, 2013.17 examples/s]Running tokenizer on dataset (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:25, 5563.88 examples/s]Running tokenizer on dataset (num_proc=1):   3%|â–Ž         | 5000/144715 [00:00<00:17, 8185.44 examples/s]Running tokenizer on dataset (num_proc=1):   5%|â–         | 7000/144715 [00:00<00:13, 10074.05 examples/s]Running tokenizer on dataset (num_proc=1):   6%|â–Œ         | 9000/144715 [00:01<00:11, 11393.40 examples/s]Running tokenizer on dataset (num_proc=1):   8%|â–Š         | 11000/144715 [00:01<00:14, 9372.34 examples/s]Running tokenizer on dataset (num_proc=1):   9%|â–‰         | 13000/144715 [00:01<00:12, 10414.17 examples/s]Running tokenizer on dataset (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:01<00:11, 11248.30 examples/s]Running tokenizer on dataset (num_proc=1):  12%|â–ˆâ–        | 17000/144715 [00:01<00:10, 11867.43 examples/s]Running tokenizer on dataset (num_proc=1):  13%|â–ˆâ–Ž        | 19000/144715 [00:01<00:10, 12351.94 examples/s]Running tokenizer on dataset (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:02<00:12, 9704.59 examples/s] Running tokenizer on dataset (num_proc=1):  16%|â–ˆâ–Œ        | 23000/144715 [00:02<00:11, 10588.13 examples/s]Running tokenizer on dataset (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:02<00:10, 11312.14 examples/s]Running tokenizer on dataset (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:02<00:09, 11890.60 examples/s]Running tokenizer on dataset (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:09, 12358.83 examples/s]Running tokenizer on dataset (num_proc=1):  21%|â–ˆâ–ˆâ–       | 31000/144715 [00:02<00:08, 12696.86 examples/s]Running tokenizer on dataset (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:03<00:11, 9885.26 examples/s] Running tokenizer on dataset (num_proc=1):  24%|â–ˆâ–ˆâ–       | 35000/144715 [00:03<00:10, 10834.33 examples/s]Running tokenizer on dataset (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:03<00:09, 11456.61 examples/s]Running tokenizer on dataset (num_proc=1):  27%|â–ˆâ–ˆâ–‹       | 39000/144715 [00:03<00:08, 11983.28 examples/s]Running tokenizer on dataset (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 41000/144715 [00:03<00:08, 12422.42 examples/s]Running tokenizer on dataset (num_proc=1):  30%|â–ˆâ–ˆâ–‰       | 43000/144715 [00:04<00:10, 9896.89 examples/s] Running tokenizer on dataset (num_proc=1):  31%|â–ˆâ–ˆâ–ˆ       | 45000/144715 [00:04<00:09, 10716.58 examples/s]Running tokenizer on dataset (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 47000/144715 [00:04<00:08, 11471.55 examples/s]Running tokenizer on dataset (num_proc=1):  34%|â–ˆâ–ˆâ–ˆâ–      | 49000/144715 [00:04<00:07, 11973.32 examples/s]Running tokenizer on dataset (num_proc=1):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 51000/144715 [00:04<00:07, 12418.33 examples/s]Running tokenizer on dataset (num_proc=1):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 53000/144715 [00:04<00:07, 12673.94 examples/s]Running tokenizer on dataset (num_proc=1):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 55000/144715 [00:05<00:08, 10033.19 examples/s]Running tokenizer on dataset (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 57000/144715 [00:05<00:08, 10778.96 examples/s]Running tokenizer on dataset (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:05<00:07, 11512.09 examples/s]Running tokenizer on dataset (num_proc=1):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 61000/144715 [00:05<00:06, 12103.30 examples/s]Running tokenizer on dataset (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:05<00:06, 12521.82 examples/s]Running tokenizer on dataset (num_proc=1):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65000/144715 [00:06<00:07, 9980.83 examples/s] Running tokenizer on dataset (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:06<00:07, 10773.79 examples/s]Running tokenizer on dataset (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69000/144715 [00:06<00:06, 11558.62 examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:06<00:06, 12126.49 examples/s]Running tokenizer on dataset (num_proc=1):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73000/144715 [00:06<00:05, 12480.34 examples/s]Running tokenizer on dataset (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:06<00:06, 10112.53 examples/s]Running tokenizer on dataset (num_proc=1):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 77000/144715 [00:07<00:06, 10975.88 examples/s]Running tokenizer on dataset (num_proc=1):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79000/144715 [00:07<00:05, 11477.60 examples/s]Running tokenizer on dataset (num_proc=1):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 81000/144715 [00:07<00:05, 12116.07 examples/s]Running tokenizer on dataset (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 83000/144715 [00:07<00:04, 12646.91 examples/s]Running tokenizer on dataset (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 85000/144715 [00:07<00:05, 10036.32 examples/s]Running tokenizer on dataset (num_proc=1):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 87000/144715 [00:07<00:05, 10920.59 examples/s]Running tokenizer on dataset (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:08<00:04, 11703.42 examples/s]Running tokenizer on dataset (num_proc=1):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 91000/144715 [00:08<00:04, 12060.41 examples/s]Running tokenizer on dataset (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:08<00:04, 12574.20 examples/s]Running tokenizer on dataset (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 95000/144715 [00:08<00:03, 12966.95 examples/s]Running tokenizer on dataset (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:08<00:04, 10706.69 examples/s]Running tokenizer on dataset (num_proc=1):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 99000/144715 [00:08<00:03, 11867.13 examples/s]Running tokenizer on dataset (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:09<00:03, 12804.19 examples/s]Running tokenizer on dataset (num_proc=1):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 103000/144715 [00:09<00:03, 13641.50 examples/s]Running tokenizer on dataset (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:09<00:02, 13945.82 examples/s]Running tokenizer on dataset (num_proc=1):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107000/144715 [00:09<00:03, 11443.32 examples/s]Running tokenizer on dataset (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:09<00:02, 12438.62 examples/s]Running tokenizer on dataset (num_proc=1):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 111000/144715 [00:09<00:02, 12886.94 examples/s]Running tokenizer on dataset (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:10<00:02, 13230.98 examples/s]Running tokenizer on dataset (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 115000/144715 [00:10<00:02, 13476.53 examples/s]Running tokenizer on dataset (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:10<00:02, 13460.31 examples/s]Running tokenizer on dataset (num_proc=1):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 119000/144715 [00:10<00:02, 10571.45 examples/s]Running tokenizer on dataset (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:10<00:02, 11432.35 examples/s]Running tokenizer on dataset (num_proc=1):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123000/144715 [00:10<00:01, 12124.17 examples/s]Running tokenizer on dataset (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:11<00:01, 12681.64 examples/s]Running tokenizer on dataset (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 127000/144715 [00:11<00:01, 13066.97 examples/s]Running tokenizer on dataset (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:11<00:01, 10256.89 examples/s]Running tokenizer on dataset (num_proc=1):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 131000/144715 [00:11<00:01, 11007.93 examples/s]Running tokenizer on dataset (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:11<00:01, 11677.10 examples/s]Running tokenizer on dataset (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 135000/144715 [00:11<00:00, 12124.77 examples/s]Running tokenizer on dataset (num_proc=1):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137000/144715 [00:12<00:00, 12473.71 examples/s]Running tokenizer on dataset (num_proc=1):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 139000/144715 [00:12<00:00, 9804.30 examples/s] Running tokenizer on dataset (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 141000/144715 [00:12<00:00, 10669.90 examples/s]Running tokenizer on dataset (num_proc=1):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 143000/144715 [00:12<00:00, 11387.98 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:12<00:00, 11878.65 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:14<00:00, 10273.99 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 144715
})
### tokenized_datasets...example [101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102]
RAM used: 1186.73 MB
merge and mask (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]merge and mask (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:24, 5881.36 examples/s]merge and mask (num_proc=1):   6%|â–Œ         | 9000/144715 [00:00<00:07, 17194.79 examples/s]merge and mask (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:00<00:04, 26372.23 examples/s]merge and mask (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:00<00:03, 33246.74 examples/s]merge and mask (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:00<00:03, 38711.26 examples/s]merge and mask (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:01<00:02, 42644.78 examples/s]merge and mask (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:01<00:03, 29984.57 examples/s]merge and mask (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 46000/144715 [00:01<00:02, 34325.93 examples/s]merge and mask (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:01<00:02, 38402.17 examples/s]merge and mask (num_proc=1):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58000/144715 [00:01<00:02, 41764.94 examples/s]merge and mask (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64000/144715 [00:01<00:01, 44493.87 examples/s]merge and mask (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 70000/144715 [00:02<00:01, 45903.41 examples/s]merge and mask (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:02<00:02, 33287.98 examples/s]merge and mask (num_proc=1):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 81000/144715 [00:02<00:01, 37551.29 examples/s]merge and mask (num_proc=1):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 87000/144715 [00:02<00:01, 40982.89 examples/s]merge and mask (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:02<00:01, 43163.12 examples/s]merge and mask (num_proc=1):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 99000/144715 [00:02<00:01, 45465.23 examples/s]merge and mask (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:02<00:00, 47744.94 examples/s]merge and mask (num_proc=1):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 111000/144715 [00:02<00:00, 49560.55 examples/s]merge and mask (num_proc=1):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 119000/144715 [00:03<00:00, 38365.49 examples/s]merge and mask (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:03<00:00, 41503.12 examples/s]merge and mask (num_proc=1):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 131000/144715 [00:03<00:00, 44654.59 examples/s]merge and mask (num_proc=1):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137000/144715 [00:03<00:00, 47433.51 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:03<00:00, 45871.84 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:05<00:00, 26946.18 examples/s]
RAM used: 1266.46 MB
padding (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1116694.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465515.02it/s]
padding (num_proc=1):   1%|          | 1000/144715 [00:00<01:45, 1360.23 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1201462.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1463469.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1209779.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1423728.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1233981.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1505493.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1246449.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1508742.45it/s]
padding (num_proc=1):   3%|â–Ž         | 5000/144715 [00:00<00:20, 6681.60 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1236528.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1496362.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1142550.80it/s]

0it [00:00, ?it/s][A1000it [00:00, 1454840.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1215035.92it/s]

0it [00:00, ?it/s][A1000it [00:00, 1493698.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1200086.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1490513.15it/s]
padding (num_proc=1):   6%|â–Œ         | 9000/144715 [00:01<00:12, 10823.30 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1203530.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1488397.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218212.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1491043.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1078781.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1491043.01it/s]
padding (num_proc=1):   8%|â–Š         | 12000/144715 [00:01<00:10, 13161.84 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1201806.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1484709.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1235071.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1247562.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1212577.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1158327.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1211526.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1441837.06it/s]
padding (num_proc=1):  11%|â–ˆ         | 16000/144715 [00:01<00:08, 15834.26 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1242388.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1499572.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1190548.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1492103.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1235799.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478429.33it/s]
padding (num_proc=1):  13%|â–ˆâ–Ž        | 19000/144715 [00:01<00:07, 16713.80 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1231084.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1480516.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1204221.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 994853.89it/s]
padding (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:01<00:09, 12393.07 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1207341.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1479994.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1230361.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497430.92it/s]

0it [00:00, ?it/s][A1000it [00:00, 1256155.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1438375.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1185836.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1446311.72it/s]
padding (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:02<00:08, 14934.98 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1238353.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1444817.09it/s]

0it [00:00, ?it/s][A1000it [00:00, 1215740.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1472719.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1206993.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1472719.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1221049.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1420353.54it/s]
padding (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:06, 16837.82 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1233981.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1487869.46it/s]

0it [00:00, ?it/s][A1000it [00:00, 1201806.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475828.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1206646.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475828.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1202495.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1470138.10it/s]
padding (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:02<00:06, 18281.77 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1252778.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1491573.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1235071.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1456355.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1234708.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1453327.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1230361.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1479994.35it/s]
padding (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:02<00:05, 19379.63 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1197004.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478429.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1198030.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1471169.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1211176.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1462959.19it/s]
padding (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:02<00:05, 18423.70 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1218565.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1467566.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1228200.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1453831.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218212.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1471685.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1197688.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476347.76it/s]
padding (num_proc=1):  30%|â–ˆâ–ˆâ–ˆ       | 44000/144715 [00:02<00:05, 19416.18 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1161535.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1457367.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1204913.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461429.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1000310.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475828.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1221760.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 900838.49it/s]
padding (num_proc=1):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 48000/144715 [00:03<00:04, 20016.66 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1214332.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1474790.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1179168.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1473754.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1206993.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1450312.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1233981.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1468593.84it/s]
padding (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:03<00:04, 20666.95 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1209081.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477908.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1165732.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445813.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1153866.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1401839.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1222829.15it/s]

0it [00:00, ?it/s][A1000it [00:00, 1467052.82it/s]
padding (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 56000/144715 [00:03<00:04, 21013.23 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1230001.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 883755.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1197004.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1450814.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1240551.32it/s]

0it [00:00, ?it/s][A1000it [00:00, 1361787.01it/s]
padding (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:03<00:05, 16005.29 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1168981.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1463469.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1169959.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1369345.09it/s]

0it [00:00, ?it/s][A1000it [00:00, 1219628.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1428577.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1189198.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1138519.00it/s]
padding (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:04<00:04, 17455.00 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1196663.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1429551.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1208384.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1416037.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218212.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1396239.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1025000.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1442332.87it/s]
padding (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:04<00:04, 18629.50 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1243493.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1452321.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1208733.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1444817.09it/s]

0it [00:00, ?it/s][A1000it [00:00, 1192241.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1440846.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1212226.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448309.39it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:04<00:03, 19518.32 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 972705.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445314.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1225329.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1464491.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1205606.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1440846.44it/s]
padding (num_proc=1):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 74000/144715 [00:04<00:03, 19927.53 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1190548.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1450814.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1180496.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1407484.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1211176.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1323124.29it/s]
padding (num_proc=1):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 77000/144715 [00:04<00:03, 17174.00 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1167679.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1458381.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1196321.73it/s]

0it [00:00, ?it/s][A1000it [00:00, 1472719.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1209779.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1452824.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1139446.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448309.39it/s]
padding (num_proc=1):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 81000/144715 [00:04<00:03, 18429.80 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1167354.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1438375.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1180164.32it/s]

0it [00:00, ?it/s][A1000it [00:00, 1434440.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1185501.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460412.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1171266.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1455850.05it/s]
padding (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 85000/144715 [00:05<00:03, 19340.85 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1161213.73it/s]

0it [00:00, ?it/s][A1000it [00:00, 1436897.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1202495.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1406540.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1155774.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1360903.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1212577.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1225687.90it/s]
padding (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:05<00:03, 15746.86 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1141617.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1435422.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1119974.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1406069.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1114320.94it/s]

0it [00:00, ?it/s][A1000it [00:00, 1260307.69it/s]

0it [00:00, ?it/s][A1000it [00:00, 1183828.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1434931.24it/s]
padding (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:05<00:03, 17204.87 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1027512.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1429551.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1118481.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1319793.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1134208.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1397169.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1148180.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1421797.97it/s]
padding (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:05<00:02, 18368.79 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1165732.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1401839.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1153231.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1419872.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1167679.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1382433.75it/s]
padding (num_proc=1):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 100000/144715 [00:06<00:02, 18929.36 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1108724.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1348217.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1095118.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1429551.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1149124.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1435422.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1171593.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1379705.26it/s]
padding (num_proc=1):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 104000/144715 [00:06<00:02, 19688.52 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1192241.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1479472.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1225687.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448809.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1202495.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1418432.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1233981.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1443822.38it/s]
padding (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108000/144715 [00:06<00:01, 20482.81 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1199057.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1458381.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1194958.40it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465003.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 791228.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478429.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1211176.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1466539.86it/s]
padding (num_proc=1):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 112000/144715 [00:06<00:01, 20906.70 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1219983.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460412.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1268310.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1479994.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1208733.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1480516.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1229640.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478429.33it/s]
padding (num_proc=1):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 116000/144715 [00:06<00:01, 21368.94 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1264487.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1506033.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1268694.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1515830.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1235071.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1466027.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1237988.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448809.67it/s]
padding (num_proc=1):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 120000/144715 [00:06<00:01, 21732.29 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1250910.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1452321.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1263725.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478429.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1140686.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1485235.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1249420.32it/s]

0it [00:00, ?it/s][A1000it [00:00, 1487869.46it/s]
padding (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 124000/144715 [00:07<00:00, 21875.48 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1204567.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477908.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1231445.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483659.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1227481.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1474272.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1262583.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1473754.04it/s]
padding (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 128000/144715 [00:07<00:00, 22120.69 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1248304.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1489983.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1248676.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1503873.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1209081.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481039.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1272543.69it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481039.55it/s]
padding (num_proc=1):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 132000/144715 [00:07<00:00, 22357.94 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1247191.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445314.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1119974.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1490513.15it/s]

0it [00:00, ?it/s][A1000it [00:00, 1244231.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481562.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 951520.87it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461939.35it/s]
padding (num_proc=1):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 136000/144715 [00:07<00:00, 17627.90 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1233981.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465515.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1263344.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1454840.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1269078.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1528536.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1244600.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1455850.05it/s]
padding (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:07<00:00, 18863.58 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1288572.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1536375.09it/s]

0it [00:00, ?it/s][A1000it [00:00, 1228200.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1492103.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1280703.51it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461429.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1198372.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1464491.62it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 144000/144715 [00:08<00:00, 19831.71 examples/s]
0it [00:00, ?it/s][A715it [00:00, 1219076.16it/s]

0it [00:00, ?it/s][A715it [00:00, 1470783.40it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:12<00:00, 11657.87 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 144715
}) padded dataset
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102]])
Column([[101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102], [101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102], [101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102], [101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102]])
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102, 102, 101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102, 102, 101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102, 102, 101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 102, 101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102, 102, 101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1760.76 MB
RAM used: 1760.76 MB
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the VALID set...
### Data samples...
 ['Why does he want to have sex with me not her?', 'When/how did you realize were not straight?'] ['Why did he chose me to have sex with?', 'When/how did you realize you were gay/bisexual? Were you in denial?']
RAM used: 1677.90 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2048
})
RAM used: 1634.86 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 1227.01 examples/s]Running tokenizer on dataset (num_proc=1):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2000/2048 [00:02<00:00, 875.97 examples/s] Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:02<00:00, 760.02 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2048
})
### tokenized_datasets...example [101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102]
RAM used: 1638.21 MB
merge and mask (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]merge and mask (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 1900.10 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 3654.17 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:01<00:00, 1863.44 examples/s]
RAM used: 1640.32 MB
padding (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 707779.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1464491.62it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 2130.47 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 847505.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1470138.10it/s]

0it [00:00, ?it/s][A48it [00:00, 689474.63it/s]

0it [00:00, ?it/s][A48it [00:00, 898779.43it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 2292.33 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2048
}) padded dataset
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102]])
Column([[101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102], [101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102], [101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102], [101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102]])
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102, 102, 101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102, 102, 101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102, 102, 101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102, 102, 101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102, 102, 101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1643.20 MB
RAM used: 1643.20 MB
############################## size of vocab 30522
### Creating model and diffusion...
dict_keys(['lr', 'batch_size', 'microbatch', 'learning_steps', 'log_interval', 'save_interval', 'eval_interval', 'ema_rate', 'resume_checkpoint', 'schedule_sampler', 'diffusion_steps', 'noise_schedule', 'timestep_respacing', 'vocab', 'use_plm_init', 'vocab_size', 'config_name', 'notes', 'data_dir', 'dataset', 'checkpoint_path', 'seq_len', 'hidden_t_dim', 'hidden_dim', 'dropout', 'use_fp16', 'fp16_scale_growth', 'seed', 'gradient_clipping', 'weight_decay', 'learn_sigma', 'use_kl', 'predict_xstart', 'rescale_timesteps', 'rescale_learned_sigmas', 'sigma_small', 'emb_scale_factor'])
initializing from pretrained bert...
BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.57.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

### The parameter count is 109380666
### Saving the hyperparameters to diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:10:25/training_args.json
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /data6/seungwoochoi/x_bridging/DiffuSeq/wandb/offline-run-20251119_011117-bz2ky8a8
### Training...
cuda:0
input_ids_mask.shape::::
torch.Size([64, 128])
x_start_mean::::
torch.Size([64, 128, 768])
Traceback (most recent call last):
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 116, in <module>
    main()
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 113, in main
    ).run_loop()
      ^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 179, in run_loop
    self.run_step(batch, cond)
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 198, in run_step
    self.forward_backward(batch, cond)
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 275, in forward_backward
    loss.backward()
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
    torch.autograd.backward(
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
    _engine_run_backward(
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [8192, 768]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 116, in <module>
[rank0]:     main()
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 113, in main
[rank0]:     ).run_loop()
[rank0]:       ^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 179, in run_loop
[rank0]:     self.run_step(batch, cond)
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 198, in run_step
[rank0]:     self.forward_backward(batch, cond)
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 275, in forward_backward
[rank0]:     loss.backward()
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/_tensor.py", line 625, in backward
[rank0]:     torch.autograd.backward(
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/autograd/__init__.py", line 354, in backward
[rank0]:     _engine_run_backward(
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/autograd/graph.py", line 841, in _engine_run_backward
[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [8192, 768]], which is output 0 of AsStridedBackward0, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /data6/seungwoochoi/x_bridging/DiffuSeq/wandb/offline-run-20251119_011117-bz2ky8a8[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20251119_011117-bz2ky8a8/logs[0m
