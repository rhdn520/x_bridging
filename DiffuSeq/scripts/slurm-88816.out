 OPENAI_LOGDIR=diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:04:16  TOKENIZERS_PARALLELISM=false python train.py   --checkpoint_path diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:04:16 --dataset qqp --data_dir /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp --vocab bert --use_plm_init bert --lr 0.0001 --batch_size 1024 --microbatch 64 --diffusion_steps 2000 --noise_schedule sqrt --schedule_sampler lossaware --resume_checkpoint none --seq_len 128 --hidden_t_dim 128 --seed 102 --hidden_dim 768 --learning_steps 50000 --save_interval 10000 --config_name bert-base-uncased --notes test-qqp20251119-01:04:16 
Logging to diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:04:16
### Creating data loader...
initializing the random embeddings Embedding(30522, 768)
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the TRAIN set...
### Data samples...
 ['Academic and Educational Advice: What can I do after completing bcom?', 'What are some good songs to make a texting lyric prank?'] ['What should I do after bcom?', 'What is a good prank lyric text to text to a friend?']
RAM used: 1022.64 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 144715
})
RAM used: 1069.78 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):   1%|          | 1000/144715 [00:00<01:18, 1830.06 examples/s]Running tokenizer on dataset (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:27, 5114.26 examples/s]Running tokenizer on dataset (num_proc=1):   3%|â–Ž         | 5000/144715 [00:00<00:18, 7553.54 examples/s]Running tokenizer on dataset (num_proc=1):   5%|â–         | 7000/144715 [00:00<00:14, 9350.15 examples/s]Running tokenizer on dataset (num_proc=1):   6%|â–Œ         | 9000/144715 [00:01<00:12, 10649.04 examples/s]Running tokenizer on dataset (num_proc=1):   8%|â–Š         | 11000/144715 [00:01<00:15, 8735.91 examples/s]Running tokenizer on dataset (num_proc=1):   9%|â–‰         | 13000/144715 [00:01<00:13, 9901.20 examples/s]Running tokenizer on dataset (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:01<00:11, 10852.18 examples/s]Running tokenizer on dataset (num_proc=1):  12%|â–ˆâ–        | 17000/144715 [00:01<00:11, 11609.24 examples/s]Running tokenizer on dataset (num_proc=1):  13%|â–ˆâ–Ž        | 19000/144715 [00:02<00:10, 12180.51 examples/s]Running tokenizer on dataset (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:02<00:12, 9627.56 examples/s] Running tokenizer on dataset (num_proc=1):  16%|â–ˆâ–Œ        | 23000/144715 [00:02<00:11, 10539.48 examples/s]Running tokenizer on dataset (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:02<00:10, 11283.33 examples/s]Running tokenizer on dataset (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:02<00:09, 11808.84 examples/s]Running tokenizer on dataset (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:09, 12286.25 examples/s]Running tokenizer on dataset (num_proc=1):  21%|â–ˆâ–ˆâ–       | 31000/144715 [00:03<00:08, 12639.17 examples/s]Running tokenizer on dataset (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:03<00:11, 9960.92 examples/s] Running tokenizer on dataset (num_proc=1):  24%|â–ˆâ–ˆâ–       | 35000/144715 [00:03<00:10, 10956.52 examples/s]Running tokenizer on dataset (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:03<00:09, 11686.33 examples/s]Running tokenizer on dataset (num_proc=1):  27%|â–ˆâ–ˆâ–‹       | 39000/144715 [00:03<00:08, 12309.24 examples/s]Running tokenizer on dataset (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 41000/144715 [00:03<00:08, 12738.90 examples/s]Running tokenizer on dataset (num_proc=1):  30%|â–ˆâ–ˆâ–‰       | 43000/144715 [00:04<00:10, 10077.84 examples/s]Running tokenizer on dataset (num_proc=1):  31%|â–ˆâ–ˆâ–ˆ       | 45000/144715 [00:04<00:09, 11048.82 examples/s]Running tokenizer on dataset (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 47000/144715 [00:04<00:08, 11844.13 examples/s]Running tokenizer on dataset (num_proc=1):  34%|â–ˆâ–ˆâ–ˆâ–      | 49000/144715 [00:04<00:07, 12456.04 examples/s]Running tokenizer on dataset (num_proc=1):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 51000/144715 [00:04<00:07, 12747.35 examples/s]Running tokenizer on dataset (num_proc=1):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 53000/144715 [00:04<00:06, 13127.18 examples/s]Running tokenizer on dataset (num_proc=1):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 55000/144715 [00:05<00:08, 10302.35 examples/s]Running tokenizer on dataset (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 57000/144715 [00:05<00:07, 11060.52 examples/s]Running tokenizer on dataset (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:05<00:07, 11838.95 examples/s]Running tokenizer on dataset (num_proc=1):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 61000/144715 [00:05<00:06, 12476.69 examples/s]Running tokenizer on dataset (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:05<00:06, 12955.30 examples/s]Running tokenizer on dataset (num_proc=1):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65000/144715 [00:06<00:07, 10837.96 examples/s]Running tokenizer on dataset (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:06<00:06, 11936.62 examples/s]Running tokenizer on dataset (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69000/144715 [00:06<00:05, 12974.91 examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:06<00:05, 13769.68 examples/s]Running tokenizer on dataset (num_proc=1):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73000/144715 [00:06<00:05, 14097.75 examples/s]Running tokenizer on dataset (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:06<00:06, 11430.00 examples/s]Running tokenizer on dataset (num_proc=1):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 77000/144715 [00:06<00:05, 12543.03 examples/s]Running tokenizer on dataset (num_proc=1):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79000/144715 [00:07<00:04, 13383.60 examples/s]Running tokenizer on dataset (num_proc=1):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 81000/144715 [00:07<00:04, 13972.78 examples/s]Running tokenizer on dataset (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 83000/144715 [00:07<00:04, 14398.97 examples/s]Running tokenizer on dataset (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 85000/144715 [00:07<00:05, 11122.68 examples/s]Running tokenizer on dataset (num_proc=1):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 87000/144715 [00:07<00:04, 11769.73 examples/s]Running tokenizer on dataset (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:07<00:04, 12448.41 examples/s]Running tokenizer on dataset (num_proc=1):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 91000/144715 [00:08<00:04, 12688.36 examples/s]Running tokenizer on dataset (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:08<00:03, 13001.14 examples/s]Running tokenizer on dataset (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 95000/144715 [00:08<00:03, 13176.99 examples/s]Running tokenizer on dataset (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:08<00:04, 10315.62 examples/s]Running tokenizer on dataset (num_proc=1):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 99000/144715 [00:08<00:04, 11224.42 examples/s]Running tokenizer on dataset (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:08<00:03, 11899.09 examples/s]Running tokenizer on dataset (num_proc=1):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 103000/144715 [00:09<00:03, 12383.55 examples/s]Running tokenizer on dataset (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:09<00:03, 12819.92 examples/s]Running tokenizer on dataset (num_proc=1):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107000/144715 [00:09<00:03, 10166.56 examples/s]Running tokenizer on dataset (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:09<00:03, 11002.51 examples/s]Running tokenizer on dataset (num_proc=1):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 111000/144715 [00:09<00:02, 11652.45 examples/s]Running tokenizer on dataset (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:09<00:02, 12036.58 examples/s]Running tokenizer on dataset (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 115000/144715 [00:10<00:02, 12500.31 examples/s]Running tokenizer on dataset (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:10<00:02, 12807.64 examples/s]Running tokenizer on dataset (num_proc=1):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 119000/144715 [00:10<00:02, 9905.47 examples/s] Running tokenizer on dataset (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:10<00:02, 10784.47 examples/s]Running tokenizer on dataset (num_proc=1):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123000/144715 [00:10<00:01, 11447.77 examples/s]Running tokenizer on dataset (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:10<00:01, 11990.50 examples/s]Running tokenizer on dataset (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 127000/144715 [00:11<00:01, 12379.51 examples/s]Running tokenizer on dataset (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:11<00:01, 9770.25 examples/s] Running tokenizer on dataset (num_proc=1):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 131000/144715 [00:11<00:01, 10627.49 examples/s]Running tokenizer on dataset (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:11<00:01, 11409.55 examples/s]Running tokenizer on dataset (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 135000/144715 [00:11<00:00, 11897.08 examples/s]Running tokenizer on dataset (num_proc=1):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137000/144715 [00:12<00:00, 12323.57 examples/s]Running tokenizer on dataset (num_proc=1):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 139000/144715 [00:12<00:00, 9879.39 examples/s] Running tokenizer on dataset (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 141000/144715 [00:12<00:00, 10652.85 examples/s]Running tokenizer on dataset (num_proc=1):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 143000/144715 [00:12<00:00, 11378.63 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:12<00:00, 11894.55 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:14<00:00, 10259.85 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 144715
})
### tokenized_datasets...example [101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102]
RAM used: 1186.85 MB
merge and mask (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]merge and mask (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:25, 5610.78 examples/s]merge and mask (num_proc=1):   6%|â–Œ         | 9000/144715 [00:00<00:08, 16545.09 examples/s]merge and mask (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:00<00:05, 25592.22 examples/s]merge and mask (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:00<00:03, 32472.59 examples/s]merge and mask (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:00<00:03, 38026.56 examples/s]merge and mask (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:01<00:02, 42120.26 examples/s]merge and mask (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:01<00:03, 30069.04 examples/s]merge and mask (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 46000/144715 [00:01<00:02, 34475.03 examples/s]merge and mask (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:01<00:02, 38104.09 examples/s]merge and mask (num_proc=1):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58000/144715 [00:01<00:02, 41513.13 examples/s]merge and mask (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64000/144715 [00:01<00:01, 44328.40 examples/s]merge and mask (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 70000/144715 [00:02<00:01, 46419.79 examples/s]merge and mask (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:02<00:01, 34851.82 examples/s]merge and mask (num_proc=1):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84000/144715 [00:02<00:01, 38721.98 examples/s]merge and mask (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 90000/144715 [00:02<00:01, 42195.39 examples/s]merge and mask (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96000/144715 [00:02<00:01, 45130.65 examples/s]merge and mask (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 102000/144715 [00:02<00:00, 47575.78 examples/s]merge and mask (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108000/144715 [00:02<00:00, 48824.76 examples/s]merge and mask (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114000/144715 [00:03<00:00, 50390.78 examples/s]merge and mask (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122000/144715 [00:03<00:00, 39320.97 examples/s]merge and mask (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 128000/144715 [00:03<00:00, 42549.65 examples/s]merge and mask (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134000/144715 [00:03<00:00, 45310.04 examples/s]merge and mask (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:03<00:00, 46854.54 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:05<00:00, 26929.23 examples/s]
RAM used: 1266.41 MB
padding (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1065896.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1104345.44it/s]
padding (num_proc=1):   1%|          | 1000/144715 [00:00<01:47, 1335.39 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1187515.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1393456.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1165084.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1398101.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1166704.87it/s]

0it [00:00, ?it/s][A1000it [00:00, 1466027.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1146924.80it/s]

0it [00:00, ?it/s][A1000it [00:00, 1447309.87it/s]
padding (num_proc=1):   3%|â–Ž         | 5000/144715 [00:00<00:21, 6614.43 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1173888.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1398101.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1117884.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1362671.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1131760.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1370687.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1165084.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1407484.56it/s]
padding (num_proc=1):   6%|â–Œ         | 9000/144715 [00:01<00:12, 10788.75 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1106968.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1444319.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1183494.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1379705.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1122972.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1386546.78it/s]
padding (num_proc=1):   8%|â–Š         | 12000/144715 [00:01<00:10, 13170.69 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1156411.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1392069.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1093690.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1391145.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1137284.16it/s]

0it [00:00, ?it/s][A1000it [00:00, 1425664.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1119675.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1396704.63it/s]
padding (num_proc=1):  11%|â–ˆ         | 16000/144715 [00:01<00:08, 15902.14 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1134515.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1360461.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1129018.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1455344.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1168329.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1439363.07it/s]
padding (num_proc=1):  13%|â–ˆâ–Ž        | 19000/144715 [00:01<00:07, 16834.72 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1123574.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1409850.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1165408.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 978834.07it/s]
padding (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:01<00:10, 12340.63 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1097697.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1367559.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1194618.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1434931.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1147866.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478950.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1204913.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1459395.96it/s]
padding (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:02<00:08, 14950.75 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1237622.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478950.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1178175.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1479994.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1190211.12it/s]

0it [00:00, ?it/s][A1000it [00:00, 1469108.23it/s]

0it [00:00, ?it/s][A1000it [00:00, 1161857.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1432970.28it/s]
padding (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:06, 17010.83 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1185836.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1468593.84it/s]

0it [00:00, ?it/s][A1000it [00:00, 1210826.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483659.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1217151.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481562.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1227840.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476867.61it/s]
padding (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:02<00:05, 18682.71 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1230001.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476867.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1183828.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1474272.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1194618.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1466027.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1226046.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1489454.55it/s]
padding (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:02<00:05, 19934.07 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1217151.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465003.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1165732.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1447809.46it/s]

0it [00:00, ?it/s][A1000it [00:00, 1211526.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1434931.24it/s]
padding (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:02<00:05, 18794.89 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1206993.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1380159.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1237622.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475828.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1224614.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1491573.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1190548.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1482610.11it/s]
padding (num_proc=1):  30%|â–ˆâ–ˆâ–ˆ       | 44000/144715 [00:02<00:05, 19885.96 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1216092.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1457874.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1184162.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1450312.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1210128.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1491043.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1195639.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1452321.33it/s]
padding (num_proc=1):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 48000/144715 [00:03<00:04, 20759.90 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1157049.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1457874.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1157688.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461939.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1217151.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477908.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1199400.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448809.67it/s]
padding (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:03<00:04, 21178.17 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1159288.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1473754.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218565.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1457367.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1162501.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460920.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1211876.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1403716.20it/s]
padding (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 56000/144715 [00:03<00:04, 21585.61 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1170939.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 903944.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1199057.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1444319.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1227122.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1421797.97it/s]
padding (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:03<00:05, 16613.93 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 982042.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1391145.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1222472.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1412699.23it/s]

0it [00:00, ?it/s][A1000it [00:00, 1202495.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1482610.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1139756.52it/s]

0it [00:00, ?it/s][A1000it [00:00, 1424211.88it/s]
padding (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:03<00:04, 18145.62 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1209779.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1440846.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1207341.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1422762.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1209779.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1458888.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1173560.16it/s]

0it [00:00, ?it/s][A1000it [00:00, 1429551.47it/s]
padding (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:04<00:04, 19377.68 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1227122.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465003.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1183828.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1427605.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1209779.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1401839.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1212577.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1423245.33it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:04<00:03, 20338.73 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1223542.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1409850.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1208733.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1436897.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1206299.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461939.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1185501.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1423728.45it/s]
padding (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:04<00:03, 20919.90 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1205952.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1457367.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1222472.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1492103.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1175863.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460920.93it/s]
padding (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:04<00:03, 17891.89 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1127198.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1452321.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1180828.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1429551.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1209430.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1466027.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1160892.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460920.93it/s]
padding (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82000/144715 [00:04<00:03, 19056.03 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1201117.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478429.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1188861.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1459395.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1140686.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1459395.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1212927.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1459903.93it/s]
padding (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 86000/144715 [00:05<00:02, 20050.53 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1143173.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1388382.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1230361.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445813.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1226763.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1216798.38it/s]
padding (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:05<00:03, 15919.82 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 952601.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1482610.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1161535.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1452824.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1199057.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1441837.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1206646.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1131149.95it/s]
padding (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:05<00:02, 17638.44 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1200430.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465515.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1162501.11it/s]

0it [00:00, ?it/s][A1000it [00:00, 1437389.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1195639.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1444319.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1173231.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1446810.62it/s]
padding (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:05<00:02, 18995.52 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1192580.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448809.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1216092.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1425664.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1181161.36it/s]

0it [00:00, ?it/s][A1000it [00:00, 1454335.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1208036.87it/s]

0it [00:00, ?it/s][A1000it [00:00, 1421797.97it/s]
padding (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:05<00:02, 19974.81 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1175863.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1450312.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1193598.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1463980.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1144733.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1376535.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 961334.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1405126.97it/s]
padding (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:06<00:01, 20626.80 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1189198.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1456355.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1171920.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1446311.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1187851.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1434440.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1160892.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 1435422.31it/s]
padding (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:06<00:01, 21190.14 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1175204.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1409376.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1188861.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1361787.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1164760.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1459903.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1177844.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1416994.59it/s]
padding (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:06<00:01, 21581.27 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1094261.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1416994.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1099136.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1393456.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1179500.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1315241.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1186507.50it/s]

0it [00:00, ?it/s][A1000it [00:00, 1359579.90it/s]
padding (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:06<00:01, 21754.91 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1098848.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1421797.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1153231.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1422280.09it/s]

0it [00:00, ?it/s][A1000it [00:00, 1209081.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1427119.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1166380.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1346918.43it/s]
padding (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:06<00:01, 21963.12 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1216798.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1419392.22it/s]

0it [00:00, ?it/s][A1000it [00:00, 1212226.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476867.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1219628.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478429.33it/s]

0it [00:00, ?it/s][A1000it [00:00, 974739.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1435422.31it/s]
padding (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:07<00:00, 22132.63 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1224971.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1460920.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1216445.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1458888.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1223542.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1480516.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1220693.83it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461939.35it/s]
padding (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:07<00:00, 22474.59 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1227481.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461429.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1210128.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461429.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1255403.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1535250.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1195298.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1468593.84it/s]
padding (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:07<00:00, 22680.61 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1224971.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1475309.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1210477.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1464491.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 924466.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1411748.23it/s]
padding (num_proc=1):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 136000/144715 [00:07<00:00, 17410.30 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1217858.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1489983.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1149124.38it/s]

0it [00:00, ?it/s][A1000it [00:00, 1489454.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1150700.69it/s]

0it [00:00, ?it/s][A1000it [00:00, 1556905.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1221404.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476347.76it/s]
padding (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:07<00:00, 18973.50 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1235435.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1522433.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1217151.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1466027.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1283839.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1530767.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1213980.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476867.61it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 144000/144715 [00:07<00:00, 20160.25 examples/s]
0it [00:00, ?it/s][A715it [00:00, 1202939.17it/s]

0it [00:00, ?it/s][A715it [00:00, 1485352.83it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:12<00:00, 11858.11 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 144715
}) padded dataset
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102]])
Column([[101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102], [101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102], [101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102], [101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102]])
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102, 102, 101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102, 102, 101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102, 102, 101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 102, 101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102, 102, 101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1761.07 MB
RAM used: 1761.07 MB
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the VALID set...
### Data samples...
 ['Why does he want to have sex with me not her?', 'When/how did you realize were not straight?'] ['Why did he chose me to have sex with?', 'When/how did you realize you were gay/bisexual? Were you in denial?']
RAM used: 1678.50 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2048
})
RAM used: 1635.46 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 1197.03 examples/s]Running tokenizer on dataset (num_proc=1):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2000/2048 [00:02<00:00, 886.35 examples/s] Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:02<00:00, 768.71 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2048
})
### tokenized_datasets...example [101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102]
RAM used: 1638.82 MB
merge and mask (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]merge and mask (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 1733.45 examples/s]merge and mask (num_proc=1):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2000/2048 [00:00<00:00, 3339.23 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:01<00:00, 1674.80 examples/s]
RAM used: 1640.93 MB
padding (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 721042.46it/s]

0it [00:00, ?it/s][A1000it [00:00, 829733.73it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 3283.40 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1086325.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1091130.07it/s]

0it [00:00, ?it/s][A48it [00:00, 655786.94it/s]

0it [00:00, ?it/s][A48it [00:00, 923516.48it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 2852.92 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2048
}) padded dataset
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102]])
Column([[101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102], [101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102], [101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102], [101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102]])
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102, 102, 101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102, 102, 101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102, 102, 101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102, 102, 101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102, 102, 101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1643.80 MB
RAM used: 1643.80 MB
############################## size of vocab 30522
### Creating model and diffusion...
dict_keys(['lr', 'batch_size', 'microbatch', 'learning_steps', 'log_interval', 'save_interval', 'eval_interval', 'ema_rate', 'resume_checkpoint', 'schedule_sampler', 'diffusion_steps', 'noise_schedule', 'timestep_respacing', 'vocab', 'use_plm_init', 'vocab_size', 'config_name', 'notes', 'data_dir', 'dataset', 'checkpoint_path', 'seq_len', 'hidden_t_dim', 'hidden_dim', 'dropout', 'use_fp16', 'fp16_scale_growth', 'seed', 'gradient_clipping', 'weight_decay', 'learn_sigma', 'use_kl', 'predict_xstart', 'rescale_timesteps', 'rescale_learned_sigmas', 'sigma_small', 'emb_scale_factor'])
initializing from pretrained bert...
BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.57.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

### The parameter count is 109380666
### Saving the hyperparameters to diffusion_models/diffuseq_qqp_h768_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251119-01:04:16/training_args.json
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /data6/seungwoochoi/x_bridging/DiffuSeq/wandb/offline-run-20251119_010509-d18yipyz
### Training...
cuda:0
Traceback (most recent call last):
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 116, in <module>
    main()
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 113, in main
    ).run_loop()
      ^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 179, in run_loop
    self.run_step(batch, cond)
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 198, in run_step
    self.forward_backward(batch, cond)
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 260, in forward_backward
    losses = compute_losses()
             ^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 1014, in training_losses
    return super().training_losses(self._wrap_model(model), *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 196, in training_losses
    return self.training_losses_seq2seq(model, *args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 671, in training_losses_seq2seq
    x_start_mean = model.model.module.get_cls_conditioned_embeds(input_ids_x)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/transformer_model.py", line 112, in get_cls_conditioned_embeds
    print(self.input_transformers(input_ids).last_hidden_state[:,0,:])
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 650, in forward
    layer_outputs = layer_module(
                    ^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/modeling_layers.py", line 94, in __call__
    return super().__call__(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 558, in forward
    self_attention_outputs = self.attention(
                             ^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 488, in forward
    self_outputs = self.self(
                   ^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 230, in forward
    batch_size, seq_length, _ = hidden_states.shape
    ^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: not enough values to unpack (expected 3, got 2)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 116, in <module>
[rank0]:     main()
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train.py", line 113, in main
[rank0]:     ).run_loop()
[rank0]:       ^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 179, in run_loop
[rank0]:     self.run_step(batch, cond)
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 198, in run_step
[rank0]:     self.forward_backward(batch, cond)
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/train_util.py", line 260, in forward_backward
[rank0]:     losses = compute_losses()
[rank0]:              ^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 1014, in training_losses
[rank0]:     return super().training_losses(self._wrap_model(model), *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 196, in training_losses
[rank0]:     return self.training_losses_seq2seq(model, *args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/gaussian_diffusion.py", line 671, in training_losses_seq2seq
[rank0]:     x_start_mean = model.model.module.get_cls_conditioned_embeds(input_ids_x)
[rank0]:                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/x_bridging/DiffuSeq/diffuseq/transformer_model.py", line 112, in get_cls_conditioned_embeds
[rank0]:     print(self.input_transformers(input_ids).last_hidden_state[:,0,:])
[rank0]:           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 650, in forward
[rank0]:     layer_outputs = layer_module(
[rank0]:                     ^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/modeling_layers.py", line 94, in __call__
[rank0]:     return super().__call__(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 558, in forward
[rank0]:     self_attention_outputs = self.attention(
[rank0]:                              ^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 488, in forward
[rank0]:     self_outputs = self.self(
[rank0]:                    ^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/utils/deprecation.py", line 172, in wrapped_func
[rank0]:     return func(*args, **kwargs)
[rank0]:            ^^^^^^^^^^^^^^^^^^^^^
[rank0]:   File "/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py", line 230, in forward
[rank0]:     batch_size, seq_length, _ = hidden_states.shape
[rank0]:     ^^^^^^^^^^^^^^^^^^^^^^^^^
[rank0]: ValueError: not enough values to unpack (expected 3, got 2)
[1;34mwandb[0m: 
[1;34mwandb[0m: You can sync this run to the cloud by running:
[1;34mwandb[0m: [1mwandb sync /data6/seungwoochoi/x_bridging/DiffuSeq/wandb/offline-run-20251119_010509-d18yipyz[0m
[1;34mwandb[0m: Find logs at: [1;35mwandb/offline-run-20251119_010509-d18yipyz/logs[0m
