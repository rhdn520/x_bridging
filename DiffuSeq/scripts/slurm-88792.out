 OPENAI_LOGDIR=diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-23:06:14  TOKENIZERS_PARALLELISM=false python train.py   --checkpoint_path diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-23:06:14 --dataset qqp --data_dir /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp --vocab bert --use_plm_init no --lr 0.0001 --batch_size 1024 --microbatch 64 --diffusion_steps 2000 --noise_schedule sqrt --schedule_sampler lossaware --resume_checkpoint none --seq_len 128 --hidden_t_dim 128 --seed 102 --hidden_dim 128 --learning_steps 50000 --save_interval 10000 --config_name bert-base-uncased --notes test-qqp20251118-23:06:14 
Logging to diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-23:06:14
### Creating data loader...
initializing the random embeddings Embedding(30522, 128)
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the TRAIN set...
### Data samples...
 ['Academic and Educational Advice: What can I do after completing bcom?', 'What are some good songs to make a texting lyric prank?'] ['What should I do after bcom?', 'What is a good prank lyric text to text to a friend?']
RAM used: 946.84 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 144715
})
RAM used: 993.93 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):   1%|          | 1000/144715 [00:00<01:09, 2074.26 examples/s]Running tokenizer on dataset (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:24, 5816.19 examples/s]Running tokenizer on dataset (num_proc=1):   3%|â–Ž         | 5000/144715 [00:00<00:16, 8636.76 examples/s]Running tokenizer on dataset (num_proc=1):   5%|â–         | 7000/144715 [00:00<00:12, 10735.17 examples/s]Running tokenizer on dataset (num_proc=1):   6%|â–Œ         | 9000/144715 [00:00<00:11, 12228.94 examples/s]Running tokenizer on dataset (num_proc=1):   8%|â–Š         | 11000/144715 [00:01<00:12, 10338.90 examples/s]Running tokenizer on dataset (num_proc=1):   9%|â–‰         | 13000/144715 [00:01<00:11, 11704.35 examples/s]Running tokenizer on dataset (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:01<00:10, 12735.89 examples/s]Running tokenizer on dataset (num_proc=1):  12%|â–ˆâ–        | 17000/144715 [00:01<00:09, 13526.20 examples/s]Running tokenizer on dataset (num_proc=1):  13%|â–ˆâ–Ž        | 19000/144715 [00:01<00:08, 14218.38 examples/s]Running tokenizer on dataset (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:01<00:10, 11569.62 examples/s]Running tokenizer on dataset (num_proc=1):  16%|â–ˆâ–Œ        | 23000/144715 [00:02<00:09, 12586.49 examples/s]Running tokenizer on dataset (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:02<00:08, 13441.74 examples/s]Running tokenizer on dataset (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:02<00:08, 13941.24 examples/s]Running tokenizer on dataset (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:07, 14508.96 examples/s]Running tokenizer on dataset (num_proc=1):  21%|â–ˆâ–ˆâ–       | 31000/144715 [00:02<00:07, 14756.37 examples/s]Running tokenizer on dataset (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:02<00:10, 11090.84 examples/s]Running tokenizer on dataset (num_proc=1):  24%|â–ˆâ–ˆâ–       | 35000/144715 [00:03<00:09, 11970.15 examples/s]Running tokenizer on dataset (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:03<00:08, 12531.73 examples/s]Running tokenizer on dataset (num_proc=1):  27%|â–ˆâ–ˆâ–‹       | 39000/144715 [00:03<00:08, 12967.79 examples/s]Running tokenizer on dataset (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 41000/144715 [00:03<00:07, 13259.89 examples/s]Running tokenizer on dataset (num_proc=1):  30%|â–ˆâ–ˆâ–‰       | 43000/144715 [00:03<00:09, 10396.56 examples/s]Running tokenizer on dataset (num_proc=1):  31%|â–ˆâ–ˆâ–ˆ       | 45000/144715 [00:03<00:08, 11190.77 examples/s]Running tokenizer on dataset (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 47000/144715 [00:04<00:08, 12011.98 examples/s]Running tokenizer on dataset (num_proc=1):  34%|â–ˆâ–ˆâ–ˆâ–      | 49000/144715 [00:04<00:07, 12566.32 examples/s]Running tokenizer on dataset (num_proc=1):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 51000/144715 [00:04<00:07, 13061.07 examples/s]Running tokenizer on dataset (num_proc=1):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 53000/144715 [00:04<00:06, 13308.27 examples/s]Running tokenizer on dataset (num_proc=1):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 55000/144715 [00:04<00:08, 10282.87 examples/s]Running tokenizer on dataset (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 57000/144715 [00:04<00:07, 11172.13 examples/s]Running tokenizer on dataset (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:05<00:07, 11974.54 examples/s]Running tokenizer on dataset (num_proc=1):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 61000/144715 [00:05<00:06, 12527.12 examples/s]Running tokenizer on dataset (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:05<00:06, 12987.25 examples/s]Running tokenizer on dataset (num_proc=1):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 65000/144715 [00:05<00:07, 10141.98 examples/s]Running tokenizer on dataset (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:05<00:07, 10992.79 examples/s]Running tokenizer on dataset (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 69000/144715 [00:05<00:06, 11648.99 examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:06<00:06, 12174.49 examples/s]Running tokenizer on dataset (num_proc=1):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 73000/144715 [00:06<00:05, 12377.66 examples/s]Running tokenizer on dataset (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:06<00:07, 9958.21 examples/s] Running tokenizer on dataset (num_proc=1):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 77000/144715 [00:06<00:06, 10960.32 examples/s]Running tokenizer on dataset (num_proc=1):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 79000/144715 [00:06<00:05, 11763.27 examples/s]Running tokenizer on dataset (num_proc=1):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 81000/144715 [00:06<00:05, 12336.32 examples/s]Running tokenizer on dataset (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 83000/144715 [00:07<00:04, 12742.84 examples/s]Running tokenizer on dataset (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 85000/144715 [00:07<00:05, 10153.75 examples/s]Running tokenizer on dataset (num_proc=1):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 87000/144715 [00:07<00:05, 11111.39 examples/s]Running tokenizer on dataset (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:07<00:04, 11735.46 examples/s]Running tokenizer on dataset (num_proc=1):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 91000/144715 [00:07<00:04, 12235.28 examples/s]Running tokenizer on dataset (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:07<00:04, 12739.68 examples/s]Running tokenizer on dataset (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 95000/144715 [00:08<00:03, 13108.83 examples/s]Running tokenizer on dataset (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:08<00:04, 10232.16 examples/s]Running tokenizer on dataset (num_proc=1):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 99000/144715 [00:08<00:04, 11006.80 examples/s]Running tokenizer on dataset (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:08<00:03, 11796.01 examples/s]Running tokenizer on dataset (num_proc=1):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 103000/144715 [00:08<00:03, 12559.54 examples/s]Running tokenizer on dataset (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:08<00:02, 13324.56 examples/s]Running tokenizer on dataset (num_proc=1):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 107000/144715 [00:09<00:03, 11175.71 examples/s]Running tokenizer on dataset (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:09<00:02, 12255.05 examples/s]Running tokenizer on dataset (num_proc=1):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 111000/144715 [00:09<00:02, 13189.12 examples/s]Running tokenizer on dataset (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:09<00:02, 13960.10 examples/s]Running tokenizer on dataset (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 115000/144715 [00:09<00:02, 14591.01 examples/s]Running tokenizer on dataset (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:09<00:01, 15044.85 examples/s]Running tokenizer on dataset (num_proc=1):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 119000/144715 [00:10<00:02, 11991.71 examples/s]Running tokenizer on dataset (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:10<00:01, 13024.93 examples/s]Running tokenizer on dataset (num_proc=1):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 123000/144715 [00:10<00:01, 13810.47 examples/s]Running tokenizer on dataset (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:10<00:01, 14479.43 examples/s]Running tokenizer on dataset (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 127000/144715 [00:10<00:01, 14927.00 examples/s]Running tokenizer on dataset (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:10<00:01, 12001.03 examples/s]Running tokenizer on dataset (num_proc=1):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 131000/144715 [00:10<00:01, 12941.94 examples/s]Running tokenizer on dataset (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:11<00:00, 13821.76 examples/s]Running tokenizer on dataset (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 135000/144715 [00:11<00:00, 14177.25 examples/s]Running tokenizer on dataset (num_proc=1):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 137000/144715 [00:11<00:00, 14615.46 examples/s]Running tokenizer on dataset (num_proc=1):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 139000/144715 [00:11<00:00, 11726.86 examples/s]Running tokenizer on dataset (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 141000/144715 [00:11<00:00, 12374.14 examples/s]Running tokenizer on dataset (num_proc=1):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 143000/144715 [00:11<00:00, 12789.87 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:11<00:00, 13101.80 examples/s]Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:13<00:00, 10903.45 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 144715
})
### tokenized_datasets...example [101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102]
RAM used: 1110.89 MB
merge and mask (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]merge and mask (num_proc=1):   2%|â–         | 3000/144715 [00:00<00:24, 5764.04 examples/s]merge and mask (num_proc=1):   6%|â–Œ         | 9000/144715 [00:00<00:07, 17035.17 examples/s]merge and mask (num_proc=1):  10%|â–ˆ         | 15000/144715 [00:00<00:04, 26452.84 examples/s]merge and mask (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:00<00:03, 33602.15 examples/s]merge and mask (num_proc=1):  19%|â–ˆâ–Š        | 27000/144715 [00:00<00:02, 39401.92 examples/s]merge and mask (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:01<00:02, 43693.27 examples/s]merge and mask (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:01<00:03, 29893.06 examples/s]merge and mask (num_proc=1):  32%|â–ˆâ–ˆâ–ˆâ–      | 46000/144715 [00:01<00:02, 34553.96 examples/s]merge and mask (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:01<00:02, 38959.90 examples/s]merge and mask (num_proc=1):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 58000/144715 [00:01<00:02, 42628.48 examples/s]merge and mask (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 64000/144715 [00:01<00:01, 45592.77 examples/s]merge and mask (num_proc=1):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 70000/144715 [00:01<00:01, 47444.69 examples/s]merge and mask (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:02<00:01, 36174.55 examples/s]merge and mask (num_proc=1):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 84000/144715 [00:02<00:01, 39943.53 examples/s]merge and mask (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 90000/144715 [00:02<00:01, 43276.00 examples/s]merge and mask (num_proc=1):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 96000/144715 [00:02<00:01, 45408.84 examples/s]merge and mask (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 102000/144715 [00:02<00:00, 47651.38 examples/s]merge and mask (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 108000/144715 [00:02<00:00, 49402.50 examples/s]merge and mask (num_proc=1):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 114000/144715 [00:02<00:00, 50719.82 examples/s]merge and mask (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 122000/144715 [00:03<00:00, 38661.82 examples/s]merge and mask (num_proc=1):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 128000/144715 [00:03<00:00, 41512.07 examples/s]merge and mask (num_proc=1):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 134000/144715 [00:03<00:00, 44457.87 examples/s]merge and mask (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:03<00:00, 46830.86 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:05<00:00, 26980.88 examples/s]
RAM used: 1190.42 MB
padding (num_proc=1):   0%|          | 0/144715 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1200430.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1571488.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1301366.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1555750.74it/s]
padding (num_proc=1):   1%|â–         | 2000/144715 [00:00<00:55, 2572.99 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1285807.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1516378.89it/s]

0it [00:00, ?it/s][A1000it [00:00, 1264868.52it/s]

0it [00:00, ?it/s][A1000it [00:00, 1462449.09it/s]

0it [00:00, ?it/s][A1000it [00:00, 1270231.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1481562.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1338750.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1590558.97it/s]
padding (num_proc=1):   4%|â–         | 6000/144715 [00:00<00:18, 7571.34 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1332371.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1586347.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1068068.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1513096.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1340890.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1583951.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 997219.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497430.92it/s]
padding (num_proc=1):   7%|â–‹         | 10000/144715 [00:01<00:11, 11629.94 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1245709.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1519124.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1329414.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1585748.20it/s]

0it [00:00, ?it/s][A1000it [00:00, 1315653.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1531326.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 976555.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1515283.24it/s]
padding (num_proc=1):  10%|â–‰         | 14000/144715 [00:01<00:08, 15032.44 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1321873.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477387.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1290555.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1539194.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1283054.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1528536.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1038965.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1526311.50it/s]
padding (num_proc=1):  12%|â–ˆâ–        | 18000/144715 [00:01<00:07, 17698.01 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1321873.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1594186.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1328993.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1507116.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1100289.61it/s]

0it [00:00, ?it/s][A1000it [00:00, 1005587.15it/s]
padding (num_proc=1):  15%|â–ˆâ–        | 21000/144715 [00:01<00:09, 13170.86 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1218920.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1517476.12it/s]

0it [00:00, ?it/s][A1000it [00:00, 1251657.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1495828.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1287386.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1493166.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1214332.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1451818.62it/s]
padding (num_proc=1):  17%|â–ˆâ–‹        | 25000/144715 [00:01<00:07, 15705.34 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1197346.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445314.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1194277.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1480516.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218920.08it/s]

0it [00:00, ?it/s][A1000it [00:00, 1550001.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1268310.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1454335.64it/s]
padding (num_proc=1):  20%|â–ˆâ–ˆ        | 29000/144715 [00:02<00:06, 17555.99 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1233618.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1473236.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1202150.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1187851.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1200086.98it/s]

0it [00:00, ?it/s][A1000it [00:00, 1476347.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1264106.09it/s]

0it [00:00, ?it/s][A1000it [00:00, 1516927.31it/s]
padding (num_proc=1):  23%|â–ˆâ–ˆâ–Ž       | 33000/144715 [00:02<00:05, 19097.23 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1312771.21it/s]

0it [00:00, ?it/s][A1000it [00:00, 1553445.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1222116.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1497430.92it/s]

0it [00:00, ?it/s][A1000it [00:00, 997931.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1470653.58it/s]

0it [00:00, ?it/s][A1000it [00:00, 1266396.14it/s]

0it [00:00, ?it/s][A1000it [00:00, 1386546.78it/s]
padding (num_proc=1):  26%|â–ˆâ–ˆâ–Œ       | 37000/144715 [00:02<00:05, 20391.00 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1181826.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1490513.15it/s]

0it [00:00, ?it/s][A1000it [00:00, 1205259.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461939.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1254652.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1468593.84it/s]
padding (num_proc=1):  28%|â–ˆâ–ˆâ–Š       | 40000/144715 [00:02<00:05, 19298.76 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1227481.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1446810.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1253902.54it/s]

0it [00:00, ?it/s][A1000it [00:00, 1546572.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1170285.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1538629.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1316479.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1550574.49it/s]
padding (num_proc=1):  30%|â–ˆâ–ˆâ–ˆ       | 44000/144715 [00:02<00:04, 20439.12 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1230723.00it/s]

0it [00:00, ?it/s][A1000it [00:00, 1436405.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1232893.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1557483.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1199400.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1432970.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1256908.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1524093.02it/s]
padding (num_proc=1):  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 48000/144715 [00:03<00:04, 21345.89 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1225687.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1505493.18it/s]

0it [00:00, ?it/s][A1000it [00:00, 1284232.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1552870.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1257285.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1525756.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1224256.86it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478950.63it/s]
padding (num_proc=1):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 52000/144715 [00:03<00:04, 22052.08 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1245709.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1508742.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1248304.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1537501.47it/s]

0it [00:00, ?it/s][A1000it [00:00, 1235071.85it/s]

0it [00:00, ?it/s][A1000it [00:00, 1514189.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1277193.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1540890.52it/s]
padding (num_proc=1):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 56000/144715 [00:03<00:03, 22581.85 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1217504.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 934351.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1169633.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1449310.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1277971.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1527979.60it/s]
padding (num_proc=1):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 59000/144715 [00:03<00:04, 17199.77 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1198372.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1542590.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1256532.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1450814.25it/s]

0it [00:00, ?it/s][A1000it [00:00, 1271386.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1483134.37it/s]

0it [00:00, ?it/s][A1000it [00:00, 1201806.30it/s]

0it [00:00, ?it/s][A1000it [00:00, 1521880.99it/s]
padding (num_proc=1):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 63000/144715 [00:03<00:04, 18742.18 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1218565.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1520226.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1220338.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1425179.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1206299.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1454335.64it/s]

0it [00:00, ?it/s][A1000it [00:00, 1272543.69it/s]

0it [00:00, ?it/s][A1000it [00:00, 1500645.44it/s]
padding (num_proc=1):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 67000/144715 [00:03<00:03, 20007.63 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1238719.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1501720.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1252778.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1442332.87it/s]

0it [00:00, ?it/s][A1000it [00:00, 1206299.68it/s]

0it [00:00, ?it/s][A1000it [00:00, 1506574.71it/s]

0it [00:00, ?it/s][A1000it [00:00, 1266013.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477908.39it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 71000/144715 [00:04<00:03, 20902.00 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1229640.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1539194.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1277582.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1517476.12it/s]

0it [00:00, ?it/s][A1000it [00:00, 1027763.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1510916.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1239817.91it/s]

0it [00:00, ?it/s][A1000it [00:00, 1478950.63it/s]
padding (num_proc=1):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 75000/144715 [00:04<00:03, 21624.52 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1288572.66it/s]

0it [00:00, ?it/s][A1000it [00:00, 1543726.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1248676.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1520226.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1194277.90it/s]

0it [00:00, ?it/s][A1000it [00:00, 1477387.81it/s]
padding (num_proc=1):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 78000/144715 [00:04<00:03, 18549.89 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1259550.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445314.96it/s]

0it [00:00, ?it/s][A1000it [00:00, 1277971.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1510916.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1210477.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1488925.81it/s]

0it [00:00, ?it/s][A1000it [00:00, 1230001.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1457874.17it/s]
padding (num_proc=1):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 82000/144715 [00:04<00:03, 19850.33 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1188188.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1373380.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1157368.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1501182.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1214684.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1461429.97it/s]

0it [00:00, ?it/s][A1000it [00:00, 1237257.82it/s]

0it [00:00, ?it/s][A1000it [00:00, 1404656.40it/s]
padding (num_proc=1):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 86000/144715 [00:04<00:02, 20753.65 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1222472.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1484184.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1260307.69it/s]

0it [00:00, ?it/s][A1000it [00:00, 1522433.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1256908.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1247191.20it/s]
padding (num_proc=1):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 89000/144715 [00:05<00:03, 16320.38 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1216092.78it/s]

0it [00:00, ?it/s][A1000it [00:00, 1502796.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1245339.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1492103.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1005346.12it/s]

0it [00:00, ?it/s][A1000it [00:00, 1512005.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1217504.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1509285.35it/s]
padding (num_proc=1):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 93000/144715 [00:05<00:02, 18079.60 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1204567.49it/s]

0it [00:00, ?it/s][A1000it [00:00, 1456861.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1224614.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1433460.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1192241.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1534127.29it/s]

0it [00:00, ?it/s][A1000it [00:00, 1186507.50it/s]

0it [00:00, ?it/s][A1000it [00:00, 1427119.43it/s]
padding (num_proc=1):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 97000/144715 [00:05<00:02, 19473.86 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1259550.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1448809.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1274864.44it/s]

0it [00:00, ?it/s][A1000it [00:00, 1532445.74it/s]

0it [00:00, ?it/s][A1000it [00:00, 1224614.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1479472.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1244600.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1430526.60it/s]
padding (num_proc=1):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 101000/144715 [00:05<00:02, 20542.68 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1242020.73it/s]

0it [00:00, ?it/s][A1000it [00:00, 1531886.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1220338.67it/s]

0it [00:00, ?it/s][A1000it [00:00, 1459903.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1202495.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1502796.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 976555.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1512005.77it/s]
padding (num_proc=1):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 105000/144715 [00:05<00:01, 21298.96 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1256532.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1435422.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1189873.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1530209.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1214684.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1468079.80it/s]

0it [00:00, ?it/s][A1000it [00:00, 1252031.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1445314.96it/s]
padding (num_proc=1):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 109000/144715 [00:06<00:01, 21889.69 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1198372.57it/s]

0it [00:00, ?it/s][A1000it [00:00, 1450312.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 979062.56it/s]

0it [00:00, ?it/s][A1000it [00:00, 1457874.17it/s]

0it [00:00, ?it/s][A1000it [00:00, 1197346.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1316479.60it/s]

0it [00:00, ?it/s][A1000it [00:00, 1125987.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1427605.17it/s]
padding (num_proc=1):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 113000/144715 [00:06<00:01, 22158.61 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1200430.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1458888.35it/s]

0it [00:00, ?it/s][A1000it [00:00, 1218565.95it/s]

0it [00:00, ?it/s][A1000it [00:00, 1480516.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1277582.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1522433.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1186843.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1500108.73it/s]
padding (num_proc=1):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 117000/144715 [00:06<00:01, 22654.29 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1249048.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1459903.93it/s]

0it [00:00, ?it/s][A1000it [00:00, 1232893.59it/s]

0it [00:00, ?it/s][A1000it [00:00, 1489454.55it/s]

0it [00:00, ?it/s][A1000it [00:00, 1248304.76it/s]

0it [00:00, ?it/s][A1000it [00:00, 1509828.65it/s]

0it [00:00, ?it/s][A1000it [00:00, 1255403.77it/s]

0it [00:00, ?it/s][A1000it [00:00, 1491573.26it/s]
padding (num_proc=1):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 121000/144715 [00:06<00:01, 22906.57 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1281486.10it/s]

0it [00:00, ?it/s][A1000it [00:00, 1441837.06it/s]

0it [00:00, ?it/s][A1000it [00:00, 1114320.94it/s]

0it [00:00, ?it/s][A1000it [00:00, 1514736.01it/s]

0it [00:00, ?it/s][A1000it [00:00, 1118481.07it/s]

0it [00:00, ?it/s][A1000it [00:00, 1465515.02it/s]

0it [00:00, ?it/s][A1000it [00:00, 1199057.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1490513.15it/s]
padding (num_proc=1):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 125000/144715 [00:06<00:00, 22932.78 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1227481.42it/s]

0it [00:00, ?it/s][A1000it [00:00, 1380159.26it/s]

0it [00:00, ?it/s][A1000it [00:00, 1216445.48it/s]

0it [00:00, ?it/s][A1000it [00:00, 1510372.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1259550.75it/s]

0it [00:00, ?it/s][A1000it [00:00, 1534688.62it/s]

0it [00:00, ?it/s][A1000it [00:00, 1206646.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1431991.81it/s]
padding (num_proc=1):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 129000/144715 [00:06<00:00, 23153.93 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1272157.72it/s]

0it [00:00, ?it/s][A1000it [00:00, 1522433.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1211876.34it/s]

0it [00:00, ?it/s][A1000it [00:00, 1515283.24it/s]

0it [00:00, ?it/s][A1000it [00:00, 1262583.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1523539.41it/s]

0it [00:00, ?it/s][A1000it [00:00, 1214684.04it/s]

0it [00:00, ?it/s][A1000it [00:00, 1443325.53it/s]
padding (num_proc=1):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 133000/144715 [00:07<00:00, 23280.48 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1229280.19it/s]

0it [00:00, ?it/s][A1000it [00:00, 1468593.84it/s]

0it [00:00, ?it/s][A1000it [00:00, 1276027.99it/s]

0it [00:00, ?it/s][A1000it [00:00, 1525201.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 977465.39it/s]

0it [00:00, ?it/s][A1000it [00:00, 1458381.08it/s]
padding (num_proc=1):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 136000/144715 [00:07<00:00, 17738.36 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1177183.27it/s]

0it [00:00, ?it/s][A1000it [00:00, 1501182.53it/s]

0it [00:00, ?it/s][A1000it [00:00, 1185166.43it/s]

0it [00:00, ?it/s][A1000it [00:00, 1467566.13it/s]

0it [00:00, ?it/s][A1000it [00:00, 1261824.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1531886.05it/s]

0it [00:00, ?it/s][A1000it [00:00, 1203185.31it/s]

0it [00:00, ?it/s][A1000it [00:00, 1456861.41it/s]
padding (num_proc=1):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 140000/144715 [00:07<00:00, 19231.34 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1284232.70it/s]

0it [00:00, ?it/s][A1000it [00:00, 1492634.88it/s]

0it [00:00, ?it/s][A1000it [00:00, 1260686.50it/s]

0it [00:00, ?it/s][A1000it [00:00, 1512551.03it/s]

0it [00:00, ?it/s][A1000it [00:00, 1253527.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1525756.28it/s]

0it [00:00, ?it/s][A1000it [00:00, 1246820.45it/s]

0it [00:00, ?it/s][A1000it [00:00, 1504413.20it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 144000/144715 [00:07<00:00, 20468.97 examples/s]
0it [00:00, ?it/s][A715it [00:00, 1267509.45it/s]

0it [00:00, ?it/s][A715it [00:00, 1498714.32it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 144715/144715 [00:11<00:00, 12196.95 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 144715
}) padded dataset
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102]])
Column([[101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102], [101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102], [101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102], [101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102], [101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102]])
Column([[101, 3834, 1998, 4547, 6040, 1024, 2054, 2064, 1045, 2079, 2044, 7678, 4647, 5358, 1029, 102, 102, 101, 2054, 2323, 1045, 2079, 2044, 4647, 5358, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2024, 2070, 2204, 2774, 2000, 2191, 1037, 3793, 2075, 13677, 26418, 1029, 102, 102, 101, 2054, 2003, 1037, 2204, 26418, 13677, 3793, 2000, 3793, 2000, 1037, 2767, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2079, 1045, 3102, 11432, 1029, 102, 102, 101, 2129, 2064, 1045, 3102, 1037, 5308, 9350, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1037, 3608, 2003, 6908, 20018, 10745, 3257, 2007, 2019, 3988, 10146, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 102, 101, 1037, 3608, 2003, 6908, 1999, 1037, 20018, 10745, 3257, 2007, 1037, 10146, 1997, 2753, 2463, 1013, 1055, 1012, 2054, 1005, 1055, 1996, 4555, 4578, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2129, 2106, 6221, 8398, 2663, 1996, 3864, 1029, 102, 102, 101, 2129, 2106, 6221, 8398, 2663, 1996, 2355, 4883, 2602, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1684.72 MB
RAM used: 1684.72 MB
############################## 
Loading text data...
############################## 
Loading dataset qqp from /home/seungwoochoi/data/x_bridging/DiffuSeq/datasets/qqp...
### Loading form the VALID set...
### Data samples...
 ['Why does he want to have sex with me not her?', 'When/how did you realize were not straight?'] ['Why did he chose me to have sex with?', 'When/how did you realize you were gay/bisexual? Were you in denial?']
RAM used: 1604.98 MB
Dataset({
    features: ['src', 'trg'],
    num_rows: 2048
})
RAM used: 1561.94 MB
Running tokenizer on dataset (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]Running tokenizer on dataset (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 1211.83 examples/s]Running tokenizer on dataset (num_proc=1):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 2000/2048 [00:02<00:00, 838.34 examples/s] Running tokenizer on dataset (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:02<00:00, 769.59 examples/s]
### tokenized_datasets Dataset({
    features: ['input_id_x', 'input_id_y'],
    num_rows: 2048
})
### tokenized_datasets...example [101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102]
RAM used: 1565.30 MB
merge and mask (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]merge and mask (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 4555.99 examples/s]merge and mask (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 4216.77 examples/s]
RAM used: 1567.41 MB
padding (num_proc=1):   0%|          | 0/2048 [00:00<?, ? examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1199400.63it/s]

0it [00:00, ?it/s][A1000it [00:00, 1575621.34it/s]
padding (num_proc=1):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 1000/2048 [00:00<00:00, 3683.85 examples/s]
0it [00:00, ?it/s][A1000it [00:00, 1253527.79it/s]

0it [00:00, ?it/s][A1000it [00:00, 1499572.40it/s]

0it [00:00, ?it/s][A48it [00:00, 687121.47it/s]

0it [00:00, ?it/s][A48it [00:00, 910980.05it/s]
padding (num_proc=1): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2048/2048 [00:00<00:00, 3497.09 examples/s]
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
Dataset({
    features: ['input_id_x', 'input_id_y', 'input_ids', 'input_mask'],
    num_rows: 2048
}) padded dataset
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102]])
Column([[101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102], [101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102], [101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102], [101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102]])
Column([[101, 2339, 2515, 2002, 2215, 2000, 2031, 3348, 2007, 2033, 2025, 2014, 1029, 102, 102, 101, 2339, 2106, 2002, 4900, 2033, 2000, 2031, 3348, 2007, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2043, 1013, 2129, 2106, 2017, 5382, 2020, 2025, 3442, 1029, 102, 102, 101, 2043, 1013, 2129, 2106, 2017, 5382, 2017, 2020, 5637, 1013, 22437, 1029, 2020, 2017, 1999, 14920, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2073, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 3899, 8902, 2140, 1029, 102, 102, 101, 2064, 2017, 2131, 1037, 8040, 9541, 3762, 20160, 9127, 2005, 2115, 3899, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2054, 2079, 2017, 2228, 1996, 3013, 7245, 1997, 24888, 7685, 2355, 7842, 2052, 2022, 1029, 102, 102, 101, 2054, 2079, 2017, 2228, 2055, 1996, 24888, 7685, 2355, 3259, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 2339, 2003, 5522, 2061, 2502, 1029, 102, 102, 101, 2339, 2038, 5522, 4961, 2000, 2022, 1037, 2107, 2312, 2103, 1029, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])
RAM used: 1570.30 MB
RAM used: 1570.30 MB
############################## size of vocab 30522
### Creating model and diffusion...
dict_keys(['lr', 'batch_size', 'microbatch', 'learning_steps', 'log_interval', 'save_interval', 'eval_interval', 'ema_rate', 'resume_checkpoint', 'schedule_sampler', 'diffusion_steps', 'noise_schedule', 'timestep_respacing', 'vocab', 'use_plm_init', 'vocab_size', 'config_name', 'notes', 'data_dir', 'dataset', 'checkpoint_path', 'seq_len', 'hidden_t_dim', 'hidden_dim', 'dropout', 'use_fp16', 'fp16_scale_growth', 'seed', 'gradient_clipping', 'weight_decay', 'learn_sigma', 'use_kl', 'predict_xstart', 'rescale_timesteps', 'rescale_learned_sigmas', 'sigma_small', 'emb_scale_factor'])
config:::
BertConfig {
  "architectures": [
    "BertForMaskedLM"
  ],
  "attention_probs_dropout_prob": 0.1,
  "classifier_dropout": null,
  "gradient_checkpointing": false,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 768,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "max_position_embeddings": 512,
  "model_type": "bert",
  "num_attention_heads": 12,
  "num_hidden_layers": 12,
  "pad_token_id": 0,
  "position_embedding_type": "absolute",
  "transformers_version": "4.57.1",
  "type_vocab_size": 2,
  "use_cache": true,
  "vocab_size": 30522
}

### The parameter count is 91225274
### Saving the hyperparameters to diffusion_models/diffuseq_qqp_h128_lr0.0001_t2000_sqrt_lossaware_seed102_test-qqp20251118-23:06:14/training_args.json
wandb: Tracking run with wandb version 0.23.0
wandb: W&B syncing is set to `offline` in this directory. Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
wandb: Run data is saved locally in /data6/seungwoochoi/x_bridging/DiffuSeq/wandb/offline-run-20251118_230701-5hln28nv
### Training...
cuda:0
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
------------------------
| grad_norm | 26.7     |
| loss      | 0.88     |
| loss_q0   | 0.876    |
| loss_q1   | 0.879    |
| loss_q2   | 0.883    |
| loss_q3   | 0.882    |
| mse       | 0.88     |
| mse_q0    | 0.876    |
| mse_q1    | 0.879    |
| mse_q2    | 0.883    |
| mse_q3    | 0.882    |
| nll       | 13.3     |
| nll_q0    | 13       |
| nll_q1    | 13.3     |
| nll_q2    | 13.6     |
| nll_q3    | 13.6     |
| samples   | 1.02e+03 |
| step      | 0        |
------------------------
/data6/seungwoochoi/.conda/envs/x_bridging/lib/python3.11/site-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 1, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
eval on validation set
---------------------------
| eval_loss    | 0.552    |
| eval_loss_q0 | 0.547    |
| eval_loss_q1 | 0.549    |
| eval_loss_q2 | 0.552    |
| eval_loss_q3 | 0.56     |
| eval_mse     | 0.552    |
| eval_mse_q0  | 0.547    |
| eval_mse_q1  | 0.549    |
| eval_mse_q2  | 0.552    |
| eval_mse_q3  | 0.56     |
| eval_nll     | 4.91     |
| eval_nll_q0  | 4.91     |
| eval_nll_q1  | 4.92     |
| eval_nll_q2  | 4.84     |
| eval_nll_q3  | 4.97     |
---------------------------
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
x_start_mean::::
torch.Size([64, 128, 128])
slurmstepd-n02: error: *** JOB 88792 ON n02 CANCELLED AT 2025-11-18T23:08:15 ***
